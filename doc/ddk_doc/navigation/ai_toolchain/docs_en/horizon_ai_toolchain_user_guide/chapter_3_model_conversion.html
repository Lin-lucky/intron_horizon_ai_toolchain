<!DOCTYPE html>
<html class="writer-html5" lang="EN" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3. Model Conversion &mdash; horizon_ai_toolchain_user_guide v1.12.3 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/custom-style.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Application Development" href="chapter_4_application_development.html" />
    <link rel="prev" title="2. Prerequisites" href="chapter_2_prerequisites.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> horizon_ai_toolchain_user_guide
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">AI Toolchain:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="chapter_1_introduction.html">1. About The Toolchain</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_2_prerequisites.html">2. Prerequisites</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">3. Model Conversion</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#general-descriptions">3.1. General Descriptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#fpm-preparations">3.2. FPM Preparations</a></li>
<li class="toctree-l2"><a class="reference internal" href="#check-the-model">3.3. Check The Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#use-the-hb-mapper-checker-command-to-check-your-model">3.3.1. Use The <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> Command To Check Your Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exception-handling">3.3.2. Exception Handling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interpret-model-check-results">3.3.3. Interpret Model Check Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#a-guide-to-optimize-the-check-results">3.3.4. A Guide To Optimize The Check Results</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#convert-the-model">3.4. Convert The Model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#convert-the-model-using-the-hb-mapper-makertbin-tool">3.4.1. Convert The Model Using The <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> Tool</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-conversion-interpretation">3.4.2. Model Conversion Interpretation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prepare-calibration-data">3.4.3. Prepare Calibration Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interpret-conversion-results">3.4.4. Interpret Conversion Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interpret-conversion-output">3.4.5. Interpret Conversion Output</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-performance-analysis-and-optimization">3.5. Model Performance Analysis And Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#use-the-hb-perf-tool-to-evaluate-model-performance">3.5.1. Use The <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> Tool To Evaluate Model Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluate-actual-model-performance-on-dev-board">3.5.2. Evaluate Actual Model Performance on Dev Board</a></li>
<li class="toctree-l3"><a class="reference internal" href="#model-performance-optimization">3.5.3. Model Performance Optimization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#check-those-performance-affecting-yaml-configuration-parameters">3.5.3.1. Check Those Performance-affecting YAML Configuration Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cpu-op-processing">3.5.3.2. CPU OP Processing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#high-performance-model-design-proposal">3.5.3.3. High Performance Model Design Proposal</a></li>
<li class="toctree-l4"><a class="reference internal" href="#bpu-high-efficiency-model-optimization">3.5.3.4. BPU High-efficiency Model Optimization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#model-accuracy-analysis-and-optimization">3.6. Model Accuracy Analysis And Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#model-accuracy-analysis">3.6.1. Model Accuracy Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="#accuracy-optimization">3.6.2. Accuracy Optimization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#apparent-accuracy-loss-over-4">3.6.2.1. Apparent Accuracy Loss (Over 4%)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#improve-model-accuracy-when-there-is-smaller-accuracy-loss">3.6.2.2. Improve Model Accuracy When There Is Smaller Accuracy Loss</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#further-improve-model-accuracy-using-the-qat-solution">3.6.3. Further Improve Model Accuracy Using The QAT Solution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#about-quantization">3.6.3.1. About Quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#about-model-conversion">3.6.3.2. About Model Conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#about-model-quantization-compilation-workflow">3.6.3.3. About Model Quantization &amp; Compilation Workflow</a></li>
<li class="toctree-l4"><a class="reference internal" href="#qat-model-quantization-compilation">3.6.3.4. QAT Model Quantization &amp; Compilation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#supported-op-list-and-restrictions">3.7. Supported OP List And Restrictions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#other-dev-tools-optional">3.8. Other Dev Tools (Optional)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pack-multiple-bin-models-into-one-file">3.8.1. Pack Multiple Bin Models Into One File</a></li>
<li class="toctree-l3"><a class="reference internal" href="#view-model-information">3.8.2. View Model Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#modify-the-nodes-of-bin-models">3.8.3. Modify The Nodes Of Bin Models</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#faq">3.9. FAQ</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#how-to-understand-the-bpu-supported-and-cpu-supported-etc-op-supporting-modes-mentioned-in-the-op-restrictions">3.9.1. How to understand the <cite>BPU supported</cite> and <cite>CPU supported</cite> etc. OP supporting modes mentioned in the OP Restrictions?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why-the-model-performance-in-dev-board-test-differ-from-that-of-in-benchmark">3.9.2. Why the model performance in dev board test differ from that of in benchmark?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#whether-symmetric-or-asymmetric-model-quantization-is-used-whether-can-support-16-bit-quantization">3.9.3. Whether symmetric or asymmetric model quantization is used? Whether can support 16-bit quantization?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#how-does-model-segmentation-affects-performance">3.9.4. How does model segmentation affects performance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#which-model-conversion-parameters-can-affect-model-s-final-performance">3.9.5. Which model conversion parameters can affect model’s final performance?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#whether-or-not-support-sparsity-optimization">3.9.6. Whether or not support sparsity optimization?</a></li>
<li class="toctree-l3"><a class="reference internal" href="#why-some-bpu-supported-ops-at-model-rear-part-run-in-cpu">3.9.7. Why some BPU supported OPs at model rear part run in CPU?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="chapter_4_application_development.html">4. Application Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_5_custom_op_development.html">5. Custom OP Development</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter_6_appendix.html">6. Appendix</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">horizon_ai_toolchain_user_guide</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li><span class="section-number">3. </span>Model Conversion</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/chapter_3_model_conversion.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="model-conversion">
<h1><span class="section-number">3. </span>Model Conversion<a class="headerlink" href="#model-conversion" title="Permalink to this headline"></a></h1>
<section id="general-descriptions">
<h2><span class="section-number">3.1. </span>General Descriptions<a class="headerlink" href="#general-descriptions" title="Permalink to this headline"></a></h2>
<p>Model Conversion refers to the very process of converting the original FPM <a class="footnote-reference brackets" href="#fpm" id="id1">1</a> into the Horizon’s HGM <a class="footnote-reference brackets" href="#hgm" id="id2">2</a>.
The original FPM (also referred to the FPM) refers to the models users trained from some open source deep learning
frameworks e.g. TensorFlow/PyTorch etc. with float32 computing accuracy; while the HGM refers to a specific model format
for Horizon’s ASIC <a class="footnote-reference brackets" href="#asic" id="id3">3</a> runtime.
Please be very clear of the terms FPM and HGM, as they appear multiple times in this chapter.</p>
<p>The complete model development process is composed by 5 major stages:
<strong>FPM Preparations</strong>, <strong>Model Validation</strong>, <strong>Model Conversion</strong>, <strong>Performance Evaluation</strong> and <strong>Accuracy Evaluation</strong>,
as shown in below diagram:</p>
<img alt="_images/model_conversion_flowchart.png" class="align-center" src="_images/model_conversion_flowchart.png" />
<p>The FPM, as the output of the <strong>FPM Preparations</strong> stage, will serve as the input of the model conversion tool.
THe FPM is usually trained based on some open source deep learning frameworks.
Note that the model must be exported into Horizon supported formats.
More information please refer to the <a class="reference internal" href="#fp-model-preparation"><span class="std std-ref">FPM Preparations</span></a> section.</p>
<p>The <strong>Model Validation</strong> stage is used for ensuring that the model can satisfy the toolchain’s requirements.
To enable users to adjust the unsupported model with the help of Horizon’s OP restrictions,
Horizon provides such tool to accomplish model validation,
with which users can point out the details of the unsupported OPs.
More information please refer to the <a class="reference internal" href="#model-check"><span class="std std-ref">Check The Model</span></a> section.</p>
<p>The <strong>Model Conversion</strong> stage converts the FPM into Horizon supported HGM.
To run models efficiently, Horizon’s model conversion tool deals with the key stages i.e.
optimization, quantization and compilation of models. Horizon’s model quantization method
has been through long-term technological and production validations and can maintain over 99%
accuracy for the majority of classic deep learning models. More details about model conversion
please refer to the <a class="reference internal" href="#id4"><span class="std std-ref">Model Conversion</span></a> section.</p>
<p>The <strong>Performance Evaluation</strong> stage contains a series of model performance evaluation tools.
Before deploying your application, users can validate whether model performance satisfy application requirements
using these tools. When model performance fails the expectation,
users can optimize their models based on Horizon’s model optimization advices.
More information please refer to the <a class="reference internal" href="#performance-evaluation"><span class="std std-ref">Model Performance Analysis And Optimization</span></a> section.</p>
<p>The <strong>Accuracy Evaluation</strong> stage contains a series of model accuracy evaluation tools.
In most cases, Horizon’s toolchain converted models can maintain almost the same accuracy as the original FPM,
therefore, users can validate model accuracy before deploying the application using the tools.
More information about evaluation please refer to the
<a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">Model Accuracy Analysis and Optimization</span></a> section.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Usually you can get qualified runtime model after model conversion,
but to ensure that the performance and accuracy of the model can satisfy the requirements of your application,
Horizon strongly recommend you to launch the performance and accuracy evaluations after each conversion.</p>
</div>
</section>
<section id="fpm-preparations">
<span id="fp-model-preparation"></span><h2><span class="section-number">3.2. </span>FPM Preparations<a class="headerlink" href="#fpm-preparations" title="Permalink to this headline"></a></h2>
<p>The open source deep learning frameworks trained FPMs are input of the model conversion tool.
Currently the conversion tool can support the following deep learning frameworks:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 30%" />
<col style="width: 13%" />
<col style="width: 12%" />
<col style="width: 15%" />
<col style="width: 9%" />
<col style="width: 21%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>FRAMEWORK</p></th>
<th class="head"><p>Caffe</p></th>
<th class="head"><p>PyTorch</p></th>
<th class="head"><p>TensorFlow</p></th>
<th class="head"><p>MXNet</p></th>
<th class="head"><p>OTHERS</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>HORIZON’S TOOLCHAIN</strong></p></td>
<td><p>supported</p></td>
<td colspan="3"><p>supported (convert into ONNX)</p></td>
<td><p>contact Horizon</p></td>
</tr>
</tbody>
</table>
<p>As shown above, the caffemodel exported from Caffe framework can be supported directly;
While the models trained from PyTorch, TensorFlow and MXNet must be first converted into ONNX before using the conversion tool.
Presently Horizon support ONNX opset10 and opset11.</p>
<p>There are standard solutions to convert models of different frameworks into ONNX, refer to below:</p>
<dl class="simple">
<dt>🔗 Pytorch2Onnx: PyTorch’s official API can support exporting models into ONNX models,</dt><dd><p>click here to see more: <a class="reference external" href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html">https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html</a>.</p>
</dd>
<dt>🔗 Tensorflow2Onnx: convert based on the <cite>onnx/tensorflow-onnx</cite> of the ONNX community,</dt><dd><p>click here to see more: <a class="reference external" href="https://github.com/onnx/tensorflow-onnx">https://github.com/onnx/tensorflow-onnx</a>.</p>
</dd>
<dt>🔗 MXNet2Onnx: MXNet’s official API can support exporting models into ONNX models,</dt><dd><p>click here to see more:
<a class="reference external" href="https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Tests/OnnxConversionTest.cs">https://github.com/dotnet/machinelearning/blob/master/test/Microsoft.ML.Tests/OnnxConversionTest.cs</a>.</p>
</dd>
<dt>🔗 More solutions to convert models of other frameworks into ONNX please click here:</dt><dd><p><a class="reference external" href="https://github.com/onnx/tutorials#converting-to-onnx-format">https://github.com/onnx/tutorials#converting-to-onnx-format</a>.</p>
</dd>
</dl>
</section>
<section id="check-the-model">
<span id="model-check"></span><h2><span class="section-number">3.3. </span>Check The Model<a class="headerlink" href="#check-the-model" title="Permalink to this headline"></a></h2>
<p>To ensure that the model can run on Horizon’s platform efficiently, all OPs in the model must comply with Horizon’s
OP restrictions. The <strong>OP Restriction</strong> section explains the details of Horizon’s supported OPs,
in which specific parameter restrictions are offered for each OP. Please refer to the:
<span class="red">supported_op_list_and_restrictions_release</span> excel table in the
<cite>horizon_xj3_open_explorer_${version}_${date}/ddk/doc/navigation/ai_toolchain/docs_cn/supported_op_list_and_restrictions/</cite>
directory. Because there are a large amount of OPs supported by Horizon, we provide the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> tool to
check details of each OP and to save users’ trouble of verifying each OP manually.</p>
<section id="use-the-hb-mapper-checker-command-to-check-your-model">
<h3><span class="section-number">3.3.1. </span>Use The <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> Command To Check Your Model<a class="headerlink" href="#use-the-hb-mapper-checker-command-to-check-your-model" title="Permalink to this headline"></a></h3>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_mapper checker --model-type <span class="si">${</span><span class="nv">model_type</span><span class="si">}</span> <span class="se">\</span>
                  --march <span class="si">${</span><span class="nv">march</span><span class="si">}</span> <span class="se">\</span>
                  --proto <span class="si">${</span><span class="nv">proto</span><span class="si">}</span> <span class="se">\</span>
                  --model <span class="si">${</span><span class="nv">caffe_model</span><span class="p">/onnx_model</span><span class="si">}</span> <span class="se">\</span>
                  --input-shape <span class="si">${</span><span class="nv">input_node</span><span class="si">}</span> <span class="si">${</span><span class="nv">input_shape</span><span class="si">}</span> <span class="se">\</span>
                  --output <span class="si">${</span><span class="nv">output</span><span class="si">}</span>
</pre></div>
</div>
</div></blockquote>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">hb_mapper</span> <span class="pre">checker：</span></span></dt>
<dd><dl class="option-list">
<dt><kbd><span class="option">--model-type</span></kbd></dt>
<dd><p>This parameter is used for specifying the input model type.
Presently it can support Caffe or ONNX model input.</p>
</dd>
<dt><kbd><span class="option">--march</span></kbd></dt>
<dd><p>This parameter is used for specifying the matched AI ASIC type.
It should be specified as <code class="docutils literal notranslate"><span class="pre">bernoulli2</span></code> for the X/J3 ASICs.</p>
</dd>
<dt><kbd><span class="option">--proto</span></kbd></dt>
<dd><p>This parameter is used for specifying the prototxt filename of Caffe model.
It is only valid when the <code class="docutils literal notranslate"><span class="pre">model-type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">caffe</span></code>.</p>
</dd>
<dt><kbd><span class="option">--model</span></kbd></dt>
<dd><p>This parameter is used for specifying your FPM name. In other words,
when the <code class="docutils literal notranslate"><span class="pre">model-type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">caffe</span></code>, it should be specified as
the caffemodel filename; while when the <code class="docutils literal notranslate"><span class="pre">model-type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">onnx</span></code>,
it should be specified as the ONNX model name.</p>
</dd>
<dt><kbd><span class="option">--input-shape</span></kbd></dt>
<dd><p>This is an optional parameter to specify the input shape of the FPM.
It should be written as: <code class="docutils literal notranslate"><span class="pre">{input_name}</span> <span class="pre">{NxHxWxC/NxCxHxW}</span></code>,
where the <code class="docutils literal notranslate"><span class="pre">input_name</span></code> and the shape must be separated by space.
For instance, let the <code class="docutils literal notranslate"><span class="pre">input_name</span></code> as <code class="docutils literal notranslate"><span class="pre">data1</span></code> and input shape as <code class="docutils literal notranslate"><span class="pre">[1,224,224,3]</span></code>,
it should be written as: <code class="docutils literal notranslate"><span class="pre">--input_shape</span> <span class="pre">data1</span> <span class="pre">1x224x224x3</span></code>.
Note that your model shape is subject to the value here
when it is different from that of the actual model shape.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">--input-shape</span></code> can contain only one name and shape combination.
In other words, users will need to configure it multiple times when there are
multiple input nodes.</p>
</div>
</dd>
<dt><kbd><span class="option">--output</span></kbd></dt>
<dd><p>This is an optional parameter to specify the log filename.
The <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> results will be dumped into your
specified log file when this parameter is specified.</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="exception-handling">
<h3><span class="section-number">3.3.2. </span>Exception Handling<a class="headerlink" href="#exception-handling" title="Permalink to this headline"></a></h3>
<p>When the FPM fails the check, the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> tool will dump Error message.
a file named hb_mapper_checker.log will be generated in current directory to provide error details.
In below example, the configuration file contains an unrecognizable OP whose type is <code class="docutils literal notranslate"><span class="pre">Accuracy</span></code>:</p>
<div class="highlight-ProtoBuf notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="o">:</span> <span class="s">&quot;data&quot;</span>
  <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Input&quot;</span>
  <span class="n">top</span><span class="o">:</span> <span class="s">&quot;data&quot;</span>
  <span class="n">input_param</span> <span class="p">{</span> <span class="n">shape</span><span class="o">:</span> <span class="p">{</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">1</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">3</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">224</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">224</span> <span class="p">}</span> <span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="o">:</span> <span class="s">&quot;Convolution1&quot;</span>
  <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Convolution&quot;</span>
  <span class="n">bottom</span><span class="o">:</span> <span class="s">&quot;data&quot;</span>
  <span class="n">top</span><span class="o">:</span> <span class="s">&quot;Convolution1&quot;</span>
  <span class="n">convolution_param</span> <span class="p">{</span>
    <span class="n">num_output</span><span class="o">:</span> <span class="mi">128</span>
    <span class="n">bias_term</span><span class="o">:</span> <span class="kc">false</span>
    <span class="n">pad</span><span class="o">:</span> <span class="mi">0</span>
    <span class="n">kernel_size</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">group</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">stride</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">weight_filler</span> <span class="p">{</span>
      <span class="n">type</span><span class="o">:</span> <span class="s">&quot;msra&quot;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="o">:</span> <span class="s">&quot;accuracy&quot;</span>
  <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Accuracy&quot;</span>
  <span class="n">bottom</span><span class="o">:</span> <span class="s">&quot;Convolution3&quot;</span>
  <span class="n">top</span><span class="o">:</span> <span class="s">&quot;accuracy&quot;</span>
  <span class="n">include</span> <span class="p">{</span>
    <span class="n">phase</span><span class="o">:</span> <span class="n">TEST</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Run <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> to check this model and you will see below in the hb_mapper_checker.log file:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ValueError: Not support layer <span class="nv">name</span><span class="o">=</span>accuracy <span class="nv">type</span><span class="o">=</span>Accuracy
</pre></div>
</div>
</section>
<section id="interpret-model-check-results">
<span id="check-result"></span><h3><span class="section-number">3.3.3. </span>Interpret Model Check Results<a class="headerlink" href="#interpret-model-check-results" title="Permalink to this headline"></a></h3>
<p>If there isn’t error message, then the model check is passed and the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> tool will
dump the following results:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="o">==============================================</span>
Node         ON   Subgraph  Type
----------------------------------------------
conv1        BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
conv2_1/dw   BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
conv2_1/sep  BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
conv2_2/dw   BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
conv2_2/sep  BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
conv3_1/dw   BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
conv3_1/sep  BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
...
</pre></div>
</div>
<p>In the above code block, each line represents the model check result of a model node and consists of 4 rows:
Node, ON, Subgraph and Type. Wherein, Node denotes node name, ON denotes the hardware to process node computing,
Subgraph denotes the subgraph to which the node belongs and Type denotes the name of Horizon’s internal implementation
that mapped the node. Horizon’s tool will divide those CPU computing OPs at the non-input and output part of the
model into 2 Subgraphs at the BPU computing unit.</p>
</section>
<section id="a-guide-to-optimize-the-check-results">
<h3><span class="section-number">3.3.4. </span>A Guide To Optimize The Check Results<a class="headerlink" href="#a-guide-to-optimize-the-check-results" title="Permalink to this headline"></a></h3>
<p>Ideally, both the non-input and the output should run in BPU, i.e. there is only one subgraph.
If there are multiple subgraphs caused by CPU OPs, the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> tool will report
the cause of CPU OPs.
In below example, the Convolution2 of the Caffe model uses a 9x9 kernel which exceeds Convolution’s OP restrictions.</p>
<div class="highlight-ProtoBuf notranslate"><div class="highlight"><pre><span></span><span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="o">:</span> <span class="s">&quot;data&quot;</span>
  <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Input&quot;</span>
  <span class="n">top</span><span class="o">:</span> <span class="s">&quot;data&quot;</span>
  <span class="n">input_param</span> <span class="p">{</span> <span class="n">shape</span><span class="o">:</span> <span class="p">{</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">1</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">3</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">224</span> <span class="n">dim</span><span class="o">:</span> <span class="mi">224</span> <span class="p">}</span> <span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="o">:</span> <span class="s">&quot;Convolution1&quot;</span>
  <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Convolution&quot;</span>
  <span class="n">bottom</span><span class="o">:</span> <span class="s">&quot;data&quot;</span>
  <span class="n">top</span><span class="o">:</span> <span class="s">&quot;Convolution1&quot;</span>
  <span class="n">convolution_param</span> <span class="p">{</span>
    <span class="n">num_output</span><span class="o">:</span> <span class="mi">128</span>
    <span class="n">bias_term</span><span class="o">:</span> <span class="kc">false</span>
    <span class="n">pad</span><span class="o">:</span> <span class="mi">0</span>
    <span class="n">kernel_size</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">group</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">stride</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">weight_filler</span> <span class="p">{</span>
      <span class="n">type</span><span class="o">:</span> <span class="s">&quot;msra&quot;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="o">:</span> <span class="s">&quot;Convolution2&quot;</span>
  <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Convolution&quot;</span>
  <span class="n">bottom</span><span class="o">:</span> <span class="s">&quot;Convolution1&quot;</span>
  <span class="n">top</span><span class="o">:</span> <span class="s">&quot;Convolution2&quot;</span>
  <span class="n">convolution_param</span> <span class="p">{</span>
    <span class="n">num_output</span><span class="o">:</span> <span class="mi">128</span>
    <span class="n">bias_term</span><span class="o">:</span> <span class="kc">false</span>
    <span class="n">pad</span><span class="o">:</span> <span class="mi">4</span>
    <span class="n">kernel_size</span><span class="o">:</span> <span class="mi">9</span>
    <span class="n">group</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">stride</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">weight_filler</span> <span class="p">{</span>
      <span class="n">type</span><span class="o">:</span> <span class="s">&quot;msra&quot;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="n">layer</span> <span class="p">{</span>
  <span class="n">name</span><span class="o">:</span> <span class="s">&quot;Convolution3&quot;</span>
  <span class="n">type</span><span class="o">:</span> <span class="s">&quot;Convolution&quot;</span>
  <span class="n">bottom</span><span class="o">:</span> <span class="s">&quot;Convolution2&quot;</span>
  <span class="n">top</span><span class="o">:</span> <span class="s">&quot;Convolution3&quot;</span>
  <span class="n">convolution_param</span> <span class="p">{</span>
    <span class="n">num_output</span><span class="o">:</span> <span class="mi">128</span>
    <span class="n">bias_term</span><span class="o">:</span> <span class="kc">false</span>
    <span class="n">pad</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">kernel_size</span><span class="o">:</span> <span class="mi">3</span>
    <span class="n">group</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">stride</span><span class="o">:</span> <span class="mi">1</span>
    <span class="n">weight_filler</span> <span class="p">{</span>
      <span class="n">type</span><span class="o">:</span> <span class="s">&quot;msra&quot;</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When checking this model using the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code>, you’ll get below kernel shape out of range warning information:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="lineno">1 </span>Layer Convolution2
<span class="lineno">2 </span>        Expected data shape range of Kernel shape is <span class="o">[[</span><span class="m">1</span>, <span class="m">2048</span><span class="o">]</span>,<span class="o">[</span><span class="m">1</span>, <span class="m">7</span><span class="o">]</span>,<span class="o">[</span><span class="m">1</span>, <span class="m">7</span><span class="o">]</span>,<span class="o">[</span><span class="m">1</span>, <span class="m">2048</span><span class="o">]]</span>, but the data shape is <span class="o">[</span><span class="m">128</span>,9,9,128<span class="o">]</span>
</pre></div>
</div>
<p>And in model check results, you’ll find that there are more than 1 subgraphs, shown as below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">===============================================</span>
Node          ON   Subgraph  Type
-----------------------------------------------
Convolution1  BPU  id<span class="o">(</span><span class="m">0</span><span class="o">)</span>     HzSQuantizedConv
Convolution2  CPU  --        Conv
Convolution3  BPU  id<span class="o">(</span><span class="m">1</span><span class="o">)</span>     HzSQuantizedConv
</pre></div>
</div>
<p>According to <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code>’s reminder,
users need to adjust the kernel shape of Convolution2 into restricted range when higher performance is required.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><strong>unless your model performance fails the expectations in the subsequent performance evaluations</strong>,
users don’t need to move the CPU OP to BPU in line with the advice here,
as multiple subgraphs won’t affect the conversion.</p>
</div>
</section>
</section>
<section id="convert-the-model">
<span id="id4"></span><h2><span class="section-number">3.4. </span>Convert The Model<a class="headerlink" href="#convert-the-model" title="Permalink to this headline"></a></h2>
<p>The model conversion stage aims to convert the FPM into the Horizon Hybrid HGM.
Through this stage, you will get a new model which can run in Horizon ASICs.
Before converting the model, please be sure that your model has passed the model check
as described in the <strong>Check The Model</strong> section.</p>
<p>Model conversion is executed by the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> tool,
in which contains some important procedures, such as: model optimization, calibration and quantization etc.,
calibration must prepare data in line with model pre-processing requirements.
To help better interpret model conversion, this section explains usage of conversion tool, calibration data preparation,
conversion internal procedure, conversion results parsing and conversion output chronologically.</p>
<section id="convert-the-model-using-the-hb-mapper-makertbin-tool">
<span id="makertbin"></span><h3><span class="section-number">3.4.1. </span>Convert The Model Using The <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code> Tool<a class="headerlink" href="#convert-the-model-using-the-hb-mapper-makertbin-tool" title="Permalink to this headline"></a></h3>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">how</span> <span class="pre">to</span> <span class="pre">use:</span></span></dt>
<dd><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_mapper makertbin --config <span class="si">${</span><span class="nv">config_file</span><span class="si">}</span>  <span class="se">\</span>
                    --model-type  <span class="si">${</span><span class="nv">model_type</span><span class="si">}</span>
</pre></div>
</div>
</dd></dl>

<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">parameters：</span></span></dt>
<dd><dl class="option-list">
<dt><kbd><span class="option">--model-type</span></kbd></dt>
<dd><p>This parameter is used for specifying the model type in conversion and
presently it can be specified as <code class="docutils literal notranslate"><span class="pre">caffe</span></code> or <code class="docutils literal notranslate"><span class="pre">onnx</span></code>.</p>
</dd>
<dt><kbd><span class="option">--config</span></kbd></dt>
<dd><p>This parameter refers to the configuration file in compilation.
It is in the YAML format and therefore has a .yaml suffix.
A complete configuration file template is shown as below:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Below configuration file is only for display, in an actual configuration file of a model,
the <code class="docutils literal notranslate"><span class="pre">caffe_model</span></code> and <code class="docutils literal notranslate"><span class="pre">onnx_model</span></code> parameters are not coexisting.
The model should be either a Caffe or a ONNX model.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># model parameters</span>
<span class="n">model_parameters</span><span class="p">:</span>
  <span class="c1"># The descriptive file of the original Caffe FPM</span>
  <span class="n">prototxt</span><span class="p">:</span> <span class="s1">&#39;***.prototxt&#39;</span>

  <span class="c1"># The original Caffe model file</span>
  <span class="n">caffe_model</span><span class="p">:</span> <span class="s1">&#39;****.caffemodel&#39;</span>

  <span class="c1"># The original ONNX model file</span>
  <span class="n">onnx_model</span><span class="p">:</span> <span class="s1">&#39;****.onnx&#39;</span>

  <span class="c1"># The target AI ASIC architecture of conversion</span>
  <span class="n">march</span><span class="p">:</span> <span class="s1">&#39;bernoulli2&#39;</span>

  <span class="c1"># The prefix of the converted model file which will run in dev board</span>
  <span class="n">output_model_file_prefix</span><span class="p">:</span> <span class="s1">&#39;mobilenetv1&#39;</span>

  <span class="c1"># The directory to where the conversion results will be saved</span>
  <span class="n">working_dir</span><span class="p">:</span> <span class="s1">&#39;./model_output_dir&#39;</span>

  <span class="c1"># To specify whether or not to enable the converted HGM&#39;s capacity to dump the intermediate layer results</span>
  <span class="n">layer_out_dump</span><span class="p">:</span> <span class="kc">False</span>

  <span class="c1"># To specify the levels of the conversion generated logs</span>
  <span class="n">log_level</span><span class="p">:</span> <span class="s1">&#39;debug&#39;</span>

<span class="c1"># input information parameters</span>
<span class="n">input_parameters</span><span class="p">:</span>
  <span class="c1"># The input node name of the FPM</span>
  <span class="n">input_name</span><span class="p">:</span> <span class="s2">&quot;data&quot;</span>

  <span class="c1"># The input data format of the original FPM (whose quantity/sequence should be the same with the input_name)</span>
  <span class="n">input_type_train</span><span class="p">:</span> <span class="s1">&#39;bgr&#39;</span>

  <span class="c1"># The input data layout of the FPM (whose quantity/sequence should be the same with the input_name)</span>
  <span class="n">input_layout_train</span><span class="p">:</span> <span class="s1">&#39;NCHW&#39;</span>

  <span class="c1"># The input data size of the FPM</span>
  <span class="n">input_shape</span><span class="p">:</span> <span class="s1">&#39;1x3x224x224&#39;</span>

  <span class="c1"># the data batch_size to be passed into neural network when actually performing neural</span>
  <span class="n">input_batch</span><span class="p">:</span> <span class="mi">1</span>

  <span class="c1"># The data pre-processing method to be added into the model</span>
  <span class="n">norm_type</span><span class="p">:</span> <span class="s1">&#39;data_mean_and_scale&#39;</span>

  <span class="c1"># The image subtracted mean values of pre-processing method;</span>
  <span class="c1"># values should be separated by space if channel mean is used</span>
  <span class="n">mean_value</span><span class="p">:</span> <span class="s1">&#39;103.94 116.78 123.68&#39;</span>

  <span class="c1"># The image scale of pre-processing method, values must be separated by space</span>
  <span class="n">scale_value</span><span class="p">:</span> <span class="s1">&#39;0.017&#39;</span>

  <span class="c1"># The input data format which the converted HGM needs to match</span>
  <span class="c1"># (whose quantity/sequence should be the same with the input_name)</span>
  <span class="n">input_type_rt</span><span class="p">:</span> <span class="s1">&#39;yuv444_128&#39;</span>

  <span class="c1"># Special input data format</span>
  <span class="n">input_space_and_range</span><span class="p">:</span> <span class="s1">&#39;regular&#39;</span>

  <span class="c1"># The input data layout which the converted HGM needs to match</span>
  <span class="c1"># (whose quantity/sequence should be the same with the input_name)</span>
  <span class="c1"># If input_type_rt is configured as nv12, then this parameter does not need to be configured</span>
  <span class="n">input_layout_rt</span><span class="p">:</span> <span class="s1">&#39;NHWC&#39;</span>

<span class="c1"># Calibration parameters</span>
<span class="n">calibration_parameters</span><span class="p">:</span>
  <span class="c1"># The directory to where the calibration samples will be saved</span>
  <span class="n">cal_data_dir</span><span class="p">:</span> <span class="s1">&#39;./calibration_data&#39;</span>

  <span class="c1"># Turn on/off automatic image calibration sample processing (skimage read; resize to input node size)</span>
  <span class="n">preprocess_on</span><span class="p">:</span> <span class="kc">False</span>

  <span class="c1"># Type of calibration algorithms</span>
  <span class="n">calibration_type</span><span class="p">:</span> <span class="s1">&#39;kl&#39;</span>

  <span class="c1"># max calibration parameter</span>
  <span class="n">max_percentile</span><span class="p">:</span> <span class="mf">1.0</span>

  <span class="c1"># Enforce an OP to run in CPU</span>
  <span class="n">run_on_cpu</span><span class="p">:</span>  <span class="p">{</span><span class="n">OP_name</span><span class="p">}</span>

  <span class="c1"># Enforce an OP to run in BPU</span>
  <span class="n">run_on_bpu</span><span class="p">:</span>  <span class="p">{</span><span class="n">OP_name</span><span class="p">}</span>

<span class="c1"># compilation parameters</span>
<span class="n">compiler_parameters</span><span class="p">:</span>
  <span class="c1"># Select compilation strategy</span>
  <span class="n">compile_mode</span><span class="p">:</span> <span class="s1">&#39;latency&#39;</span>

  <span class="c1"># Turn on/off the debug information in compilation</span>
  <span class="n">debug</span><span class="p">:</span> <span class="kc">False</span>

  <span class="c1"># number of cores to run model</span>
  <span class="n">core_num</span><span class="p">:</span> <span class="mi">1</span>

  <span class="c1"># Select the priority of model compilation</span>
  <span class="n">optimize_level</span><span class="p">:</span> <span class="s1">&#39;O3&#39;</span>

<span class="n">custom_op</span><span class="p">:</span>
  <span class="c1"># The calibration method of custom OP, the register method is recommended</span>
  <span class="n">custom_op_method</span><span class="p">:</span> <span class="n">register</span>

  <span class="c1"># The implementation file of custom OP, multiple files should be separated by &quot;;&quot;</span>
  <span class="c1"># This file can be generated by the template, please refer to the custom OP related docs for more information</span>
  <span class="n">op_register_files</span><span class="p">:</span> <span class="n">sample_custom</span><span class="o">.</span><span class="n">py</span>

  <span class="c1"># The folder in which keeps the custom OP implementation file, please use relative path</span>
  <span class="n">custom_op_dir</span><span class="p">:</span> <span class="o">./</span><span class="n">custom_op</span>
</pre></div>
</div>
<p>The parameters in the configuration file is composed by:
model parameters, input information parameters, calibration parameters and compilation parameters.
All 4 parameter sets must exist in your configuration file. Parameters can be divided
into the required and the optional, while you can leave the optional parameters unconfigured.
You can specify the parameters like this: <code class="docutils literal notranslate"><span class="pre">param_name:</span>&#160; <span class="pre">'param_value'</span></code>,
while multiple values can be separated by <code class="docutils literal notranslate"><span class="pre">';'</span></code>:
<code class="docutils literal notranslate"><span class="pre">param_name:</span>&#160; <span class="pre">'param_value1;</span> <span class="pre">param_value2;</span> <span class="pre">param_value3'</span></code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To avoid parameter sequence problems, You are strongly suggested to specify the optional parameters
(<code class="docutils literal notranslate"><span class="pre">input_name</span></code>, <code class="docutils literal notranslate"><span class="pre">input_shape</span></code> etc.) explicitly when there are multi-input models.</p>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Please note that, if set <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> to <code class="docutils literal notranslate"><span class="pre">nv12</span></code> , an odd number cannot appear in the input size of model.</p>
</div>
<p>Parameter details are listed below.
We explained them in the same sequence as above as the number is huge.</p>
<p>🛠️ <strong>Model Parameters</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 16%" />
<col style="width: 72%" />
<col style="width: 8%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>NO</strong></p></td>
<td><p><strong>PARAMETER</strong></p></td>
<td><p><strong>DESCRIPTIONS</strong></p></td>
<td><p><strong>REQ./OPT.</strong></p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">prototxt</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the prototxt filename of the floating-point Caffe model.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter must be specified when the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code>’s <code class="docutils literal notranslate"><span class="pre">model-type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">caffe</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">caffe_model</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the caffemodel filename of the floating-point Caffe model.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter must be specified when the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code>’s <code class="docutils literal notranslate"><span class="pre">model-type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">caffe</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">onnx_model</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the onnx filename of the floating-point ONNX model.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter must be specified when the <code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">makertbin</span></code>’s <code class="docutils literal notranslate"><span class="pre">model-type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">onnx</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">march</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the ASIC micro architecture to run the converted HGM.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">bernoulli2</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">bernoulli2</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: refers the the micro architecture corresponding with the X/J3 ASICs.</p>
<p>Please choose according to your own ASIC.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">output_model_file_prefix</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the prefix of the converted HGM filename.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter specifies the prefix of the converted HGM filename.</p>
</td>
<td><p>required</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">working_dir</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the directory to save the conversion results.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">model_output</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: the tool will create a new directory automatically if it doesn’t exist.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">layer_out_dump</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter determines whether or not to dump the intermediate layer results when converting the HGM.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: dumping the intermediate layer results is a debugging method, please do NOT enable it unless it is necessary.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">log_level</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the tool dumped log levels.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">debug</span></code>, <code class="docutils literal notranslate"><span class="pre">info</span></code> and <code class="docutils literal notranslate"><span class="pre">warn</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">debug</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: the <code class="docutils literal notranslate"><span class="pre">debug</span></code> log dumps details in model conversion; the <code class="docutils literal notranslate"><span class="pre">info</span></code> log dumps only critical information;</p>
<p>the <code class="docutils literal notranslate"><span class="pre">warn</span></code> log dumps those information whose priority is superior to that of warnings and errors.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">output_nodes</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies model output node(s).</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: usually the conversion tool can recognize model output nodes automatically,</p>
<p>so this parameter is used for supporting the output of some intermediate layers.</p>
<p>The values should be the node names of the model. When there are multiple values,</p>
<p>please refer to the descriptions of the afore-mentioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
<p><strong>Note that the tool will no longer automatically recognize output nodes, in other words, all output will be your specified nodes</strong>.</p>
</td>
<td><p>optional</p></td>
</tr>
</tbody>
</table>
<p>🛠️ <strong>Input Information Parameters</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 15%" />
<col style="width: 72%" />
<col style="width: 8%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>NO</strong></p></td>
<td><p><strong>PARAMETER</strong></p></td>
<td><p><strong>DESCRIPTIONS</strong></p></td>
<td><p><strong>REQ./OPT.</strong></p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_name</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input node names of the original floating-point model.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: if there is only one input node, then users don’t need to configure this parameter;</p>
<p>if there are more than one nodes, it must be configured so as to guarantee the accuracy of subsequent types and input sequence</p>
<p>calibration data. Configuration method of multiple values please refer to the afore-mentioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code> descriptions.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_type_train</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input data type of the original floating-point model.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">rgb</span></code>, <code class="docutils literal notranslate"><span class="pre">bgr</span></code>, <code class="docutils literal notranslate"><span class="pre">yuv444</span></code>, <code class="docutils literal notranslate"><span class="pre">yuv444_128</span></code>, <code class="docutils literal notranslate"><span class="pre">gray</span></code> and <code class="docutils literal notranslate"><span class="pre">featuremap</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: each input node must configure a determined input data type, when there are multiple input nodes,</p>
<p>your configured node sequence must strictly correspond with the <code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values</p>
<p>please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>. Selection of data types please refer to below</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.</p>
</td>
<td><p>required</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input data layout of the original floating-point model.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">NHWC</span></code>, <code class="docutils literal notranslate"><span class="pre">HCHW</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: each input node must configure a determined input data layout who must be the same as the layout of the</p>
<p>original floating-point model. when there are multiple input nodes, your configured node sequence must strictly correspond</p>
<p>with the <code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values please refer to descriptions of the aforementioned</p>
<p><code class="docutils literal notranslate"><span class="pre">param_value</span></code>. More about data layout please refer to below</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input data formats that the converted HGM must match.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">rgb</span></code>, <code class="docutils literal notranslate"><span class="pre">bgr</span></code>, <code class="docutils literal notranslate"><span class="pre">yuv444_128</span></code>, <code class="docutils literal notranslate"><span class="pre">nv12</span></code>, <code class="docutils literal notranslate"><span class="pre">gray</span></code> and <code class="docutils literal notranslate"><span class="pre">featuremap</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: here specifies your expected data format. It doesn’t have to be the same as the data format of the original</p>
<p>model, <strong>but note that this is the format that will actually feed into your model when running in the ASIC</strong>.</p>
<p>Each input node must configure a determined input data layout, when there are multiple input nodes,</p>
<p>your configured node sequence must strictly correspond with the <code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values</p>
<p>please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>. Selection of data types please refer to below</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.</p>
</td>
<td><p>required</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input data layout that the converted HGM must match.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">NCHW</span></code>, <code class="docutils literal notranslate"><span class="pre">NHWC</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: each input node must configure a determined input data layout and this is your expected HGM layout.</p>
<p>It is recommend to se the NHWC layout for X/J3 ASICs because the improper data layout can affect model performance.</p>
<p>If input_type_rt is configured as nv12, then this parameter does not need to be configured.</p>
<p>Each input node must configure a determined input data layout, when there are multiple input nodes,</p>
<p>your configured node sequence must strictly correspond with the <code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values</p>
<p>please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>. More about data layout please refer to below</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_space_and_range</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies special data formats.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">regular</span></code> and <code class="docutils literal notranslate"><span class="pre">bt601_video</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">regular</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: the purpose of this parameter is to deal with the YUV420 format dumped by different ISP and it will</p>
<p>ONLY become valid when the <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">nv12</span></code>. <code class="docutils literal notranslate"><span class="pre">regular</span></code> is a common YUV420 format ranged between</p>
<p><code class="docutils literal notranslate"><span class="pre">[0,255]</span></code> while <code class="docutils literal notranslate"><span class="pre">bt_601_video</span></code> is another YUV420 video format ranged between <code class="docutils literal notranslate"><span class="pre">[16,235]</span></code>. More information about bt601</p>
<p>please feel free to Google it. <strong>You don’t need to configure this parameter unless otherwise point out</strong>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_shape</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input data size of the original floating-point model.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: dimensions of shape should be separated by <code class="docutils literal notranslate"><span class="pre">x</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">1x3x224x224</span></code>. You don’t need to configure this</p>
<p>parameter unless there are more input nodes in the model, because the tool can read size information in model file</p>
<p>automatically. When there are multiple input nodes, your configured node sequence must strictly correspond with the</p>
<p><code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_batch</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input data batch size that the converted HGM must match.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">1-128</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter specifies the input data batch size that the converted HGM must match, but does not affect</p>
<p>the input data batch size of the converted onnx model. If you don’t configure this parameter, the default value is 1.</p>
<p>When there are multiple input nodes, your configured node sequence must strictly correspond with the</p>
<p><code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">norm_type</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the pre-processing method to deal with model input data.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code>, <code class="docutils literal notranslate"><span class="pre">data_mean</span></code>, <code class="docutils literal notranslate"><span class="pre">data_scale</span></code> and <code class="docutils literal notranslate"><span class="pre">no_preprocess</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: <code class="docutils literal notranslate"><span class="pre">no_preprcess</span></code> means that no pre-processing method will be used. <code class="docutils literal notranslate"><span class="pre">data_mean</span></code> means that to subtract mean</p>
<p>value. <code class="docutils literal notranslate"><span class="pre">data_scale</span></code> means that to multiply scale ratio and <code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code> means to first subtract mean value and</p>
<p>then multiply scale ratio. When there are multiple input nodes, your configured node sequence must strictly correspond with</p>
<p>the <code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values please refer to descriptions of the aforementioned</p>
<p><code class="docutils literal notranslate"><span class="pre">param_value</span></code>. Influence of this parameter please refer to below</p>
<p><a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.</p>
</td>
<td><p>required</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">mean_value</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the mean value to be subtracted in the pre-processing method.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter will become valid when the <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code> or</p>
<p><code class="docutils literal notranslate"><span class="pre">data_mean</span></code>. Each input node has 2 configuration methods. If only one value is specified, then all channels will subtract</p>
<p>the same mean value. Otherwise, you need to specify the mean values for each channel and the number of values</p>
<p>(separated by space) must correspond with the numbers of channel. Your configured number of input nodes must correspond with</p>
<p>the <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> specified node number. If there is node(s) who doesn’t need <code class="docutils literal notranslate"><span class="pre">mean</span></code> processing, it should be specified as</p>
<p><code class="docutils literal notranslate"><span class="pre">'None'</span></code>. Configuration of multiple values please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">scale_value</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the scale ratio value of the pre-processing method.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter will become valid when the <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">data_mean_and_scale</span></code> or</p>
<p><code class="docutils literal notranslate"><span class="pre">data_scale</span></code>. Each input node has 2 configuration methods. You can either specify only 1 value for all channels or</p>
<p>specify the values (separated by space) for each channel. The number of values must correspond with number of channels.</p>
<p>Your configured number of input nodes must correspond with the <code class="docutils literal notranslate"><span class="pre">norm_type</span></code> specified node number. If there is node(s)</p>
<p>who doesn’t need <code class="docutils literal notranslate"><span class="pre">scale</span></code> processing, it should be specified as <code class="docutils literal notranslate"><span class="pre">'None'</span></code>.</p>
<p>Configuration of multiple values please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
</tbody>
</table>
<p>🛠️ <strong>Calibration Parameters</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 11%" />
<col style="width: 77%" />
<col style="width: 8%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>NO</strong></p></td>
<td><p><strong>PARAMETER</strong></p></td>
<td><p><strong>DESCRIPTIONS</strong></p></td>
<td><p><strong>REQ./OPT.</strong></p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cal_data_dir</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the directory to save the calibration samples.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: the calibration data in the directory must comply with the requirements</p>
<p>of input configurations, please refer to the <a class="reference internal" href="#prepare-calibration-data"><span class="std std-ref">Prepare Calibration Data</span></a> section.</p>
<p>When there are multiple input nodes, your configured node sequence must strictly correspond with the</p>
<p><code class="docutils literal notranslate"><span class="pre">input_name</span></code> sequence. Configuration of multiple values please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
</td>
<td><p>required</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter enables automatic image calibration sample processing.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: when it is enabled, there are jpg/bmp/png etc. image data saved in the <cite>cal_data_dir</cite> directory, the tool</p>
<p>can read images using the skimage and resize images to input node(s) required size. To ensure calibration effects, it is</p>
<p>recommended to keep this parameter disabled. Influence of this parameter please refer to the</p>
<p><a class="reference internal" href="#prepare-calibration-data"><span class="std std-ref">Prepare Calibration Data</span></a> section.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">calibration_type</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies types of algorithms used in calibration.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">default</span></code>, <code class="docutils literal notranslate"><span class="pre">kl</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: Both the <code class="docutils literal notranslate"><span class="pre">kl</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span></code> are public quantization calibration algorithms. Users can learn more about</p>
<p>them from the internet. While the <code class="docutils literal notranslate"><span class="pre">default</span></code> refers to such a strategy to find out a better calibration combination by</p>
<p>searching among a series of quantization calibration parameters. ©</p>
</td>
<td><p>required</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">max_percentile</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this is the parameter of the <code class="docutils literal notranslate"><span class="pre">max</span></code> calibration method and it is used for adjusting the intercept point of the <code class="docutils literal notranslate"><span class="pre">max</span></code> calibration.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">0.0</span></code> - <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter will only become valid when the <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">max</span></code>. Typical options includes:</p>
<p>0.99999/0.99995/0.99990/0.99950/0.99900. It is recommended to firstly specify the <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> as <code class="docutils literal notranslate"><span class="pre">default</span></code>, and if the results</p>
<p>fails your expectation, configure different calibration parameters according to the</p>
<p><a class="reference internal" href="#accuracy-optimization"><span class="std std-ref">Accuracy Optimization</span></a> section.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">per_channel</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter determines whether to calibrate each channel of featuremap.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: this parameter will only become valid when the <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> is specified as non-default values.</p>
<p>You are recommended to firstly try the <code class="docutils literal notranslate"><span class="pre">default</span></code>, and if the results still fails your expectation, configure different</p>
<p>calibration parameters according to the <a class="reference internal" href="#accuracy-optimization"><span class="std std-ref">Accuracy Optimization</span></a> section.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_on_cpu</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter enforces OPs to run on CPU.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: although the performance of CPU is inferior to that of the BPU, it can provide floating-point accuracy computing capacity.</p>
<p>So you can specify this parameter to enforce some OPs to run on CPU. Configuration of multiple values</p>
<p>please refer to descriptions of the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">run_on_bpu</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter enforces OPs to run on BPU.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: to ensure the accuracy of the final quantized model, in rare cases, the conversion tool can run some BPU qualified OPs</p>
<p>on CPU. If you have higher performance requirement and are willing to spend more costs on quantizing losses, please specify this parameter</p>
<p>to enforce some OPs run on BPU. The values should be model node names, configuration of multiple values please refer to descriptions of</p>
<p>the aforementioned <code class="docutils literal notranslate"><span class="pre">param_value</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
</tbody>
</table>
<p id="compiler-parameters">🛠️ <strong>Compilation Parameters</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 5%" />
<col style="width: 12%" />
<col style="width: 74%" />
<col style="width: 9%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>NO</strong></p></td>
<td><p><strong>PARAMETER</strong></p></td>
<td><p><strong>DESCRIPTIONS</strong></p></td>
<td><p><strong>REQ./OPT.</strong></p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">compile_mode</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies compilation strategies.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">latency</span></code> or <code class="docutils literal notranslate"><span class="pre">bandwidth</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">latency</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: the <code class="docutils literal notranslate"><span class="pre">latency</span></code> strategy aims to optimize the latency time of inference;</p>
<p>while the <code class="docutils literal notranslate"><span class="pre">bandwidth</span></code> strategy aims to optimize DDR access bandwidth. It is recommended to</p>
<p>use the <code class="docutils literal notranslate"><span class="pre">latency</span></code> strategy as long as your models don’t severely exceed the expected bandwidth.</p>
</td>
<td><p>required</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">debug</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter determines whether to enable debugging information.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: When enabled, there will be debugging information appended into the results of the</p>
<p>compiled models. This parameter can be used for supporting subsequent optimization analysis. Be</p>
<p>default, you are recommended to disable it.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">core_num</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the number of core to run model.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">2</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">1</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: Horizon’s ASICs can support model inference using multiple AI accelerator cores.</p>
<p>Running models with more cores applies to bigger input size. Ideally, dual core inference speed</p>
<p>can be 1.5x single core. If your model input size is big and you have higher speed requirement,</p>
<p>try <code class="docutils literal notranslate"><span class="pre">core_num=2</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">optimize_level</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the model optimization levels.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">O0</span></code>, <code class="docutils literal notranslate"><span class="pre">O1</span></code>, <code class="docutils literal notranslate"><span class="pre">O2</span></code> and <code class="docutils literal notranslate"><span class="pre">O3</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: optimization level ranges between <code class="docutils literal notranslate"><span class="pre">O0</span></code> - <code class="docutils literal notranslate"><span class="pre">O3</span></code>. <code class="docutils literal notranslate"><span class="pre">O0</span></code> doesn’t make any optimization,</p>
<p>it has fastest compilation speed and lowest optimization level. <code class="docutils literal notranslate"><span class="pre">O1</span></code> to <code class="docutils literal notranslate"><span class="pre">O3</span></code> raise optimization level</p>
<p>gradually, so the execution speed after compilation will be shorter, yet compilation time becomes longer.</p>
<p>To guarantee the best performance in production or for performance validation purpose, it is recommended</p>
<p>to specify as <code class="docutils literal notranslate"><span class="pre">O3</span></code>, while in some procedure validations or accuracy debugging process, you can try to</p>
<p>lower optimization level so as to accelerate the speed.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">input_source</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the input source of dev board bin models.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">ddr</span></code>, <code class="docutils literal notranslate"><span class="pre">pyramid</span></code> and <code class="docutils literal notranslate"><span class="pre">resizer</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">{input_name}:</span> <span class="pre">ddr</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: this is an engineering environment option and you are recommended to configure it after</p>
<p>all model validations are complete. <code class="docutils literal notranslate"><span class="pre">ddr</span></code> indicates that data are from memory, <code class="docutils literal notranslate"><span class="pre">pyramid</span></code> and <code class="docutils literal notranslate"><span class="pre">resizer</span></code></p>
<p>indicates that data are from the ASIC hardware. More information about how to configure <code class="docutils literal notranslate"><span class="pre">pyramid</span></code> and</p>
<p><code class="docutils literal notranslate"><span class="pre">resizer</span></code> source data in engineering environment please refer to the <a class="reference external" href="../bpu_sdk_api_doc/index.html">BPU SDK API DOC</a>.</p>
<p>This parameter is a bit special, e.g., let model input name be data and data source be memory (ddr), then it should be</p>
<p>specified as <code class="docutils literal notranslate"><span class="pre">&quot;data&quot;:</span> <span class="pre">&quot;ddr&quot;</span></code>.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">max_time_per_fc</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the maximum continuous execution time (by ms) of model’s each function call.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">0-4294967295</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
<p><strong>DESCRIPTIONS</strong>: inference of the compiled directive model in the BPU are denoted by 1 or multiple function-calls. The</p>
<p>function-call is the atomic unit in BPU execution. This parameter is used for specifying the max. execution time of each</p>
<p>function-call. I.e., when execution time reaches the specified value, the higher priority model will preempt the execution</p>
<p>even if the previous function-call is not complete. In other words, when the <code class="docutils literal notranslate"><span class="pre">max_time_per_fc</span></code> is specified to a model,</p>
<p>the model will become a low priority model whose execution time can be preempted by a higher priority model. Please refer to</p>
<p>the <a class="reference internal" href="chapter_4_application_development.html#preemption"><span class="std std-ref">Model Preemption</span></a> section.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that this parameter is only used for implementing the model preemption feature and can be ignored otherwise.</p>
</div>
</td>
<td><p>optional</p></td>
</tr>
</tbody>
</table>
<p>🛠️ <strong>Custom OP Parameters</strong></p>
<table class="docutils align-center">
<colgroup>
<col style="width: 4%" />
<col style="width: 13%" />
<col style="width: 75%" />
<col style="width: 8%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>NO</strong></p></td>
<td><p><strong>PARAMETER</strong></p></td>
<td><p><strong>DESCRIPTIONS</strong></p></td>
<td><p><strong>REQ./OPT.</strong></p></td>
</tr>
<tr class="row-even"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">custom_op_method</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies custom OP strategy.</p>
<p><strong>RANGE</strong>: <code class="docutils literal notranslate"><span class="pre">register</span></code>.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: presently only can support register strategy. Details please refer to the :doc:` &lt;chapter_5_custom_op_development&gt;`.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">op_register_files</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies custom OP’s Python implementation filename.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: multiple files can be separated by <code class="docutils literal notranslate"><span class="pre">:</span></code>. Details please refer to the :doc:` &lt;chapter_5_custom_op_development&gt;`.</p>
</td>
<td><p>optional</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">custom_op_dir</span></code></p></td>
<td><p><strong>PURPOSE</strong>: this parameter specifies the path of custom OP’s Python implementation file.</p>
<p><strong>RANGE</strong>: none.</p>
<p><strong>DEFAULT VALUE</strong>: none.</p>
<p><strong>DESCRIPTIONS</strong>: please use relative path when specifying this parameter.</p>
</td>
<td><p>optional</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
</dd></dl>

</section>
<section id="model-conversion-interpretation">
<span id="conversion-interpretation"></span><h3><span class="section-number">3.4.2. </span>Model Conversion Interpretation<a class="headerlink" href="#model-conversion-interpretation" title="Permalink to this headline"></a></h3>
<p>Model conversion converts FPM into the hybrid HGM supported by Horizon’s ASICs.
To enable the HGM run in the embedding terminal efficiently, this chapter explains
the 2 key points: <strong>input data processing</strong> and <strong>model optimization and compilation</strong>.</p>
<p>In terms of <strong>Input data processing</strong>, Horizon’s edge AI computing platform can provide hardware-level schemes
for some specific input access types, but the output of these schemes may not comply with the input requirement
of your models. For example, the video processing sub-systems for video access has the abilities to crop and scale
image or optimize image quality. The output of these sub-systems are mostly in the YUV420 format, however, the
algorithm models are typically trained from more commonly used image formats, such as bgr or rgb etc.
To solve this problem, Horizon provides 2 input information descriptions for each converted model:
One of them is used for describing the original FPM input (<code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> and <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code>);
while the other is used for describing the input data of the edge platform that you are going to use (<code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> and <code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code>).</p>
<p>In terms of the <strong>frequently-used image data pre-processing</strong>, such as <strong>mean</strong> and <strong>scale</strong>,
as they don’t fit the YUV420 format in the edge platform, Horizon’s integrated them into
the models. Through the 2 above-mentioned processing, the input part of the converted HGM
is shown as below:</p>
<a class="reference internal image-reference" href="_images/input_data_process.png"><img alt="_images/input_data_process.png" class="align-center" src="_images/input_data_process.png" style="width: 899.2px; height: 273.6px;" /></a>
<p>There are only 2 types of data layouts in the above diagram: namely NCHW and NHWC.
Wherein, N denotes quantity, C denotes channel, H denotes height and W denotes width.
The distinctions of the 2 layouts lie in their different memory access features.
The NHWC layout are more often used by the TensorFlow models; while the NCHW layout is used by the Caffe models.
Although Horizon’s edge platform doesn’t restrict data layout, there are still 2 requirements:
Firstly, the <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code> must be the same as the original FPM;
Secondly, the data layout at the edge AI platform must be the same as the <code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code>,
as specifying correct data layout is the fundamental of data parse.</p>
<p>The tool can automatically add data conversion nodes according to the data formats specified by the <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code>
and <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code>. Based on Horizon’s experience in production, instead of randomly combining the data types,
we’ve opened some fixed data type combinations, as listed below:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 51%" />
<col style="width: 7%" />
<col style="width: 9%" />
<col style="width: 6%" />
<col style="width: 6%" />
<col style="width: 7%" />
<col style="width: 14%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> \ <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code></p></td>
<td><p>nv12</p></td>
<td><p>yuv444</p></td>
<td><p>rgb</p></td>
<td><p>bgr</p></td>
<td><p>gray</p></td>
<td><p>featuremap</p></td>
</tr>
<tr class="row-even"><td><p>yuv444</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>rgb</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>bgr</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-odd"><td><p>gray</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
<td><p>N</p></td>
</tr>
<tr class="row-even"><td><p>featuremap</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>N</p></td>
<td><p>Y</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the above table, the first line refers to the <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> supported data types,
while the first colum refers to those <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> supported data types.
the <strong>Y/N</strong> denotes whether or not support the conversion between <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> and <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code>.
In the final bin model, the conversion from <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> to <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> is an internal process,
you only need to focus on the <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> data format. <strong>It is of vital importance to understand</strong>
<strong>the requirement of</strong> the <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> <strong>when preparing the inference data for embedded application</strong>,
<strong>please refer to below explanations to each format of the</strong> <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code>.</p>
<blockquote>
<div><ul class="simple">
<li><p>rgb, bgr and gray are commonly used image data, while each value uses UINT8 representation.</p></li>
<li><p>yuv444 is a popular image format, note that each value uses UINT8 representation.</p></li>
<li><p>NV12 is a popular YUV420 image format, each value uses UINT8 representation.</p></li>
<li><p>One special case with NV12 is to specify the <code class="docutils literal notranslate"><span class="pre">bt601_video</span></code> of the <code class="docutils literal notranslate"><span class="pre">input_space_and_range</span></code>
(refer to the aforementioned descriptions of the <code class="docutils literal notranslate"><span class="pre">input_space_and_range</span></code>),
compared with typical NV12 format, the range if values change from [0,255] to [16,235],
and each value still uses UINT8 representation.</p></li>
<li><p>Featuremap applies to the situation when all the above-mentioned formats fails to satisfy your requirement,
this type only requires that your data should be 4D, and each value uses float32 representation.
It often applies to application scenarios such as: radar or audio model processing.</p></li>
</ul>
</div></blockquote>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The above-mentioned <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> and <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> come with the Toolchain’s processing procedures,
if you can be sure that no format conversion is required, then specify the 2 <code class="docutils literal notranslate"><span class="pre">input_type</span></code> as the same, so that
the Toolchain will process in a straight-through manner which won’t affect models’ actual performance in execution.</p>
<p>Similarly, data pre-processing also comes with the Toolchain, you only need to disable it via the <code class="docutils literal notranslate"><span class="pre">norm_type</span></code>
parameter if you require no pre-processing as it won’t affect models’ actual performance in execution.</p>
</div>
<p>In terms of <strong>Model Optimization and Compilation</strong>, the important steps, including:
parse, optimization, calibration and compilation of models are accomplished in this stage.
Please refer to below workflow chart:</p>
<a class="reference internal image-reference" href="_images/model_optimization.png"><img alt="_images/model_optimization.png" class="align-center" src="_images/model_optimization.png" style="width: 844.1999999999999px; height: 515.1999999999999px;" /></a>
<p>As shown above, the <strong>model parse stage</strong> converts Caffe FPM into ONNX FPM and determines whether or not to add
data pre-processing node(s) into the original FPM according to the configuration parameters. The output of
this stage is an original_float_model.onnx, whose computing accuracy is still float32, and a data pre-processing
node is added into the input.</p>
<p>Ideally this pre-processing node should be able to complete the conversion from <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> to <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code>.
Actually the conversion of model types relies also on the ASIC hardware, but the ONNX model doesn’t contain the hardware
conversion part, so the actual ONNX input is an intermediate format which matches the processing results of the
<code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code>. Data layout (NCHW/NHWC) keeps the same as the original FPM’s input layout.
Each <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> has a specific corresponding intermediate format, as shown below:</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 19%" />
<col style="width: 19%" />
<col style="width: 14%" />
<col style="width: 14%" />
<col style="width: 16%" />
<col style="width: 19%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>nv12</strong></p></td>
<td><p><strong>yuv444</strong></p></td>
<td><p><strong>rgb</strong></p></td>
<td><p><strong>bgr</strong></p></td>
<td><p><strong>gray</strong></p></td>
<td><p>featuremap</p></td>
</tr>
<tr class="row-even"><td><p>yuv444_128</p></td>
<td><p>yuv444_128</p></td>
<td><p>RGB_128</p></td>
<td><p>BGR_128</p></td>
<td><p>GRAY_128</p></td>
<td><p>featuremap</p></td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The bold characters in the first line of the table denotes the data type specified by the <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code>.
While the second line refers to the specific intermediate type corresponding to <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> specified data type,
which is also the input type of original_float_model.onnx. Below lists explanation of each data type:</p>
<ul class="simple">
<li><p>yuv444_128 is the results of subtracting 128 from yuv444 data. Each value uses float32 representation.</p></li>
<li><p>RGB_128 is the results of subtracting 128 from RGB data. Each value uses float32 representation.</p></li>
<li><p>BGR_128 is the results of subtracting 128 from BGR data. Each value uses float32 representation.</p></li>
<li><p>GRAY_128 is the results of subtracting 128 from gray data. Each value uses float32 representation.</p></li>
<li><p>Featuremap is 4D tensor data and each value uses float32 representation.</p></li>
</ul>
</div>
<p>The <strong>Model Optimization Stage</strong> implements some OP optimizing strategies which apply to Horizon’s edge platform, e.g.
fusing BN into Conv etc. The output of this stage is an optimized_float_model.onnx using float32 computing accuracy.
The optimization doesn’t affect models’ computing results and model input data must be the same as the aforementioned
original_float_model.</p>
<p>The <strong>Model Calibration Stage</strong> uses your provided calibration data to compute the required quantization threshold
parameters who directly will enter the quantization stage and won’t generate new model status.</p>
<p>The <strong>Model Quantization Stage</strong> uses the calibration stage generated parameters to complete model quantization.
The output of this stage is a quantized_model.onnx whose computing accuracy has become int8. It can be used for
evaluating the accuracy loss in model quantization and it requires that the basic input data formats must still
be the same as the <code class="docutils literal notranslate"><span class="pre">original_float_model</span></code>. Yet, compared with the <code class="docutils literal notranslate"><span class="pre">original_float_model</span></code>, the layouts and
value representations have changed. See below:</p>
<ul class="simple">
<li><p>Data layout is NHWC.</p></li>
<li><p>The input data type will be INT8 when the value of <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> is specified as non <code class="docutils literal notranslate"><span class="pre">featuremap</span></code>;
Otherwise, the input data type will be float32 when the value of <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">featuremap</span></code>.</p></li>
</ul>
<p>The <strong>Model Compilation Stage</strong> uses Horizon’s model compiler to convert the quantized models into the directives
and data supported by Horizon’s edge AI platform. the output of this stage is a ***.bin model who, as the final
results of conversion, will run in Horizon’s embedding runtime.</p>
</section>
<section id="prepare-calibration-data">
<span id="id5"></span><h3><span class="section-number">3.4.3. </span>Prepare Calibration Data<a class="headerlink" href="#prepare-calibration-data" title="Permalink to this headline"></a></h3>
<p>When converting models, 100 samples are required at the calibration stage, each is an independent data file.
To ensure the accuracy of the converted models, these calibration samples better come from the training or
validation dataset when training the models. In addition, please try NOT to use rare samples, e.g. single colored images
or those images who don’t contain any detection or classification targets in them.</p>
<p>The ON and OFF states of the aforementioned <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> parameter in conversion configuration file
respectively correspond to 2 different requirements for pre-processing samples.</p>
<p>When the <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> is OFF, you need to pre-process the samples coming from the training/validation datasets
in the same way as before inference. The data type (the aforementioned <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code>),
size (the aforementioned <code class="docutils literal notranslate"><span class="pre">input_shape</span></code>) and layout (the aforementioned <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code>) of the pre-processed
calibration samples will be the same as the original models. Save them one-by-one into binary as independent files.
For example, there is an ImageNet trained original classification FPM with only one input node, it should be described
as below:</p>
<ul class="simple">
<li><p>Input type: <code class="docutils literal notranslate"><span class="pre">BGR</span></code></p></li>
<li><p>Input layout: <code class="docutils literal notranslate"><span class="pre">NCHW</span></code></p></li>
<li><p>Input size: <code class="docutils literal notranslate"><span class="pre">1x3x224x224</span></code></p></li>
</ul>
<p>Use the validation dataset as the pre-processing dataset of inference, as shown below:</p>
<ol class="arabic simple">
<li><p>Uniformly scale the image and resize the shorter side to 256.</p></li>
<li><p>Get 224x224 image using the <code class="docutils literal notranslate"><span class="pre">center_crop</span></code> method.</p></li>
<li><p>Subtract mean value by the channel.</p></li>
<li><p>Image data multiply scale ratio.</p></li>
</ol>
<p>Below shows the sample code in which the <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> is OFF
(to avoid prolonged code, some simple transformer implementations are ignored, please kindly implement in your own way):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># this sample uses skimage, mind the differences when using opencv
# note that there are not mean subtraction or scale multiplication implementations in below transformers
# note that the mean and scale operations are fused into the model as previously described in the norm_type/mean_values/scale_values
def data_transformer():
  transformers = [
  # uniformly scale the image and resize the shorter side to 256
  ShortSideResizeTransformer(short_size=256),
  # get 224x224 image using the CenterCrop
  CenterCropTransformer(crop_size=224),
  # read the NHWC layout results using the skimage and convert into the model required NCHW layout
  HWC2CHWTransformer(),
  # read the RGB channel sequence results using the skimage and convert into the model required BGR
  RGB2BGRTransformer(),
  # read the value range between [0.0,1.0] using the skimage and adjust into the model required value range
  ScaleTransformer(scale_value=255)
  ]

  return transformers

# the src_image refers to the source images in sample dataset
# the dst_file refers to the filename to save the final sample datasets
def convert_image(src_image, dst_file, transformers)：
  image = skimage.img_as_float(skimage.io.imread(src_file))
  for trans in transformers:
  image = trans(image)
  # model specified input_type_train BGR value type is UINT8
  image = image.astype(np.uint8)
  # save samples into data files as binary
  image.tofile(dst_file)

if __name__ == &#39;__main__&#39;:
  # here refers to the original sample images, pseudo-code
  src_images = [&#39;ILSVRC2012_val_00000001.JPEG&#39;，...]
  # here denotes the filename (no restrictions on suffix) of the final samples, pseudo-code
  # calibration_data_bgr_f32 refers to your specified cal_data_dir in the configuration file
  dst_files = [&#39;./calibration_data_bgr_f32/ILSVRC2012_val_00000001.bgr&#39;，...]

  transformers = data_transformer()
  for src_image, dst_file in zip(src_images, dst_files):
  convert_image(src_image, dst_file, transformers)
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When the <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> is ON, use skimage supported image formats to read samples.
After reading the images, the conversion tool will resize them into model input node required size
so as to serve as calibration input. Such operations are easier, but cannot guarantee the accuracy
of the quantized models. Therefore, we strongly recommend you to use the <code class="docutils literal notranslate"><span class="pre">pre-process_on</span></code> OFF method
as mentioned previously.</p>
</div>
</section>
<section id="interpret-conversion-results">
<h3><span class="section-number">3.4.4. </span>Interpret Conversion Results<a class="headerlink" href="#interpret-conversion-results" title="Permalink to this headline"></a></h3>
<p>This chapter explains the messages of successful model conversions and analyzes the unsuccessful model conversion.
You can judge whether the model conversion is successful from the <code class="docutils literal notranslate"><span class="pre">makertbin</span></code>’s status, similarity messages and
the output of the <cite>working_dir</cite> folder. When the conversion is successful, the console should print below message:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">2021</span>-04-21 <span class="m">11</span>:13:08,337 INFO Convert to runtime bin file successfully!
<span class="m">2021</span>-04-21 <span class="m">11</span>:13:08,337 INFO End Model Convert
</pre></div>
</div>
<p>The similarity message should be printed before the status message in <code class="docutils literal notranslate"><span class="pre">makertbin</span></code>’s console:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">======================================================================</span>
Node    ON   Subgraph  Type     Cosine Similarity  Threshold
----------------------------------------------------------------------
...    ...     ...     ...       <span class="m">0</span>.999936           <span class="m">127</span>.000000
...    ...     ...     ...       <span class="m">0</span>.999868           <span class="m">2</span>.557209
...    ...     ...     ...       <span class="m">0</span>.999268           <span class="m">2</span>.133924
...    ...     ...     ...       <span class="m">0</span>.996023           <span class="m">3</span>.251645
...    ...     ...     ...       <span class="m">0</span>.996656           <span class="m">4</span>.495638
</pre></div>
</div>
<p>As shown above, the <strong>Node</strong>, <strong>ON</strong>, <strong>Subgraph</strong>, and <strong>Type</strong> in the header should be the same as the
<code class="docutils literal notranslate"><span class="pre">hb_mapper</span> <span class="pre">checker</span></code> tool’s console output. Please refer to the <a class="reference internal" href="#check-result"><span class="std std-ref">Interpret Model Check Results</span></a>
section. The <strong>Threshold</strong> refers to the calibration threshold values of each layer,
it is used for error feedback for Horizon’s technical support, so normally users don’t need to pay attention.
The <strong>Cosine Similarity</strong> reflects the cosine similarity between the original FPM and the quantized model results
in those nodes specified by the <strong>Node</strong>.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note that the Cosine Similarity field serves only as a reference to reflect the post-quantization data stability,
it cannot directly tell the model accuracy loss. Usually, there are apparent accuracy loss when the similarity of
the output nodes is below 0.8. Because the similarity is not directly related to the accuracy, if you are intended to
get model accuracy, please refer to the <a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">Model Accuracy Analysis and Optimization</span></a> section.</p>
</div>
<p>The outputs of model conversion are in the <code class="docutils literal notranslate"><span class="pre">working_dir</span></code> specified path, you are ought to find the following files
in that path:</p>
<ul class="simple">
<li><p>***_original_float_model.onnx</p></li>
<li><p>***_optimized_float_model.onnx</p></li>
<li><p>***_quantized_model.onnx</p></li>
<li><p>***.bin</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The *** refers to your specified model file prefix by the <code class="docutils literal notranslate"><span class="pre">output_model_file_prefix</span></code> parameter.</p>
</div>
<p>The <a class="reference internal" href="#conversion-output"><span class="std std-ref">Interpret Conversion Output</span></a> section explains the function of each output.
Yet, to avoid potential model conversion caused problems to be extended into the embedding terminal,
<span class="raw-html"><br /></span>
👍 <strong>we strongly recommend you to proceed the procedures as described in the</strong>
<a class="reference internal" href="#model-check"><span class="std std-ref">Check The Model</span></a> <strong>and</strong>
<strong>the</strong> <a class="reference internal" href="#performance-evaluation"><span class="std std-ref">Model Performance Analysis And Optimization</span></a> <strong>sections</strong>. 👍</p>
<p>If any of the above-mentioned 3 outputs (as conversion checking methods) is missing,
there must be something wrong with the conversion. In such cases, the <code class="docutils literal notranslate"><span class="pre">makertbin</span></code> tool
will print error messages to your console. Please refer to below example, in which
the <code class="docutils literal notranslate"><span class="pre">prototxt</span></code> and <code class="docutils literal notranslate"><span class="pre">caffe_model</span></code> parameters for Caffe model are not configured:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="m">2021</span>-04-21 <span class="m">14</span>:45:34,085 ERROR Key <span class="s1">&#39;model_parameters&#39;</span> error:
Missing keys: <span class="s1">&#39;caffe_model&#39;</span>, <span class="s1">&#39;prototxt&#39;</span>
<span class="m">2021</span>-04-21 <span class="m">14</span>:45:34,085 ERROR yaml file parse failed. Please double check your input
<span class="m">2021</span>-04-21 <span class="m">14</span>:45:34,085 ERROR exception in command: makertbin
</pre></div>
</div>
<p>In case the console cannot print error details, remember to specify the aforementioned <code class="docutils literal notranslate"><span class="pre">log_level</span></code> as <code class="docutils literal notranslate"><span class="pre">debug</span></code>,
then redo the conversion and you will be able to find the error cause in the hb_mapper_makertbin.log file in your
current working directory.</p>
<p>If the above-mentioned 2 steps still cannot help you find out the root cause, please feel free to raise a question in
Horizon’s official technical community (<a class="reference external" href="https://developer.horizon.ai/">https://developer.horizon.ai/</a>), we will get back to you in 24 hours.</p>
</section>
<section id="interpret-conversion-output">
<span id="conversion-output"></span><h3><span class="section-number">3.4.5. </span>Interpret Conversion Output<a class="headerlink" href="#interpret-conversion-output" title="Permalink to this headline"></a></h3>
<p>As previously described, the outputs of successful model conversion are composed by the 4 parts.
This chapter explains the functions of outputs.</p>
<ul class="simple">
<li><p>***_original_float_model.onnx</p></li>
<li><p>***_optimized_float_model.onnx</p></li>
<li><p>***_quantized_model.onnx</p></li>
<li><p>***.bin</p></li>
</ul>
<p>The generating process of the ***_original_float_model.onnx please refer to the
<a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.
The computing accuracy of this model is the same as the original FPM, an important change
is that some data pre-processing computations are added into the model in order to fit
Horizon’s edge platform. Although ususally you don’t need to use this model, in case an error
takes place, you can send this model to Horizon’s engineer in order to help solving the problem.</p>
<p>The generating process of the ***_optimized_float_model.onnx please refer to the
<a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.
This model has been optimized at the OP level, e.g. OP fusion. Through visualized
comparision with the original_float model, you can find out some OP structural changes
which won’t affect the computing accuracy of models. Although usually you don’t need to
use this model, in case an error takes place, you can send this model to Horizon’s
engineer in order to help solving the problem.</p>
<p>The generating process of the ***_quantized_model.onnx please refer to the
<a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.
This model has completed the calibration and quantization processes and you can
find out the accuracy loss after conversion from this model. It is also the required
model for accuracy validation, please refer to the
<a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">Model Accuracy Analysis and Optimization</span></a> section.</p>
<p>The ***.bin refer to the model to run in Horizon’s AI ASICs.
After reading <a class="reference internal" href="chapter_4_application_development.html"><span class="doc">Chapter 4: Application Development</span></a>,
you will be able to quickly deploy and run it.
However, to ensure that the model performance and accuracy can satisfy your expectations,
👍 <strong>you are strongly recommended to proceed the performance and accuracy analysis procedures</strong>
<strong>ln line with the descriptions in the</strong> <a class="reference internal" href="#id4"><span class="std std-ref">Convert The Model</span></a> <strong>and</strong>
<a class="reference internal" href="#accuracy-evaluation"><span class="std std-ref">Model Accuracy Analysis And Optimization</span></a> <strong>sections before diving</strong>
<strong>into application development and development.</strong> 👍</p>
</section>
</section>
<section id="model-performance-analysis-and-optimization">
<span id="performance-evaluation"></span><h2><span class="section-number">3.5. </span>Model Performance Analysis And Optimization<a class="headerlink" href="#model-performance-analysis-and-optimization" title="Permalink to this headline"></a></h2>
<p>This chapter elaborates how to evaluate model performance using Horizon provided tools.
These tools can obtain model performance as if they were running in the ASICs.
In case that the evaluation results fail your expectations at this stage,
Instead of leaving the performance problems to the application development stage,
👍 <strong>you’re strongly recommended to solve the performance problems according to</strong>
<strong>Horizon’s optimization advices.</strong> 👍</p>
<section id="use-the-hb-perf-tool-to-evaluate-model-performance">
<span id="hb-perf"></span><h3><span class="section-number">3.5.1. </span>Use The <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> Tool To Evaluate Model Performance<a class="headerlink" href="#use-the-hb-perf-tool-to-evaluate-model-performance" title="Permalink to this headline"></a></h3>
<p>Horizon’s <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> tool takes the conversion generated ***.bin as input to directly obtain the
expected on-board performance of the model. Tool usage please refer to below contents:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_perf  ***.bin
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are intended to analyse the packed model after running the <code class="docutils literal notranslate"><span class="pre">pack</span></code> command,
you need to add a <code class="docutils literal notranslate"><span class="pre">-p</span></code> parameter and run <code class="docutils literal notranslate"><span class="pre">hb_perf</span> <span class="pre">-p</span> <span class="pre">***.bin</span></code>.
About the <code class="docutils literal notranslate"><span class="pre">pack</span></code> command please refer to the <a class="reference internal" href="#other-tools"><span class="std std-ref">Other Optional Tools</span></a>  section.</p>
</div>
<p>The ***.bin refers to the model conversion generated bin model.
A <cite>hb_perf_result</cite> directory should be generated after running the above command,
in which contains analysis results as HTML files.
Below is an example of analysis results based on MobileNet.
Wherein, the mobilenetv1_224x224_nv12.html is the mainpage of analysis results.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_perf_result/
└── mobilenetv1_224x224_nv12
    ├── MOBILENET_subgraph_0.html
    ├── MOBILENET_subgraph_0.json
    ├── mobilenetv1_224x224_nv12
    ├── mobilenetv1_224x224_nv12.html
    ├── mobilenetv1_224x224_nv12.png
    └── temp.hbm
</pre></div>
</div>
<p>Open the mainpage using your web browser, you’ll see the following:</p>
<img alt="_images/hb_mapper_perf_2.png" class="align-center" src="_images/hb_mapper_perf_2.png" />
<p>Analysis results consist of 3 parts: <strong>Model Performance Summary</strong>, <strong>Details</strong> and <strong>BIN Model Structure</strong>.
<strong>Model Performance Summary</strong> is the overall performance evaluation result of the bin model.
Wherein, the indicators represent respectively:</p>
<ul class="simple">
<li><p>Model Name——model name.</p></li>
<li><p>Model Latency(ms)——model’s overall time consumption of single frame computing (by millisecond).</p></li>
<li><p>Model Frame Rate(fps)——model’s overall frame rate (by fps).</p></li>
<li><p>Model DDR Occupation(Mb per frame)——model’s overall DDR occupation during execution (by MB/frame).</p></li>
<li><p>Loaded Bytes per Frame——model’s data loading volume per frame during execution.</p></li>
<li><p>Stored Bytes per Frame——model’s data storing volume per frame during execution.</p></li>
</ul>
<p>You need to understand the concept of subgraph before getting to know <strong>Details</strong> and <strong>BIN Model Structure</strong>.
If there are CPU computing OPs in the non-input or output part of the model,
model conversion tool divide the OP’s continuous BPU computing part into 2 independent subgraphs.
More information please refer to the <a class="reference internal" href="#model-check"><span class="std std-ref">Check The Model</span></a> section.</p>
<p><strong>Details</strong> refer to the detailed information of each model’s BPU subgraph(s).
The indicators of each subgraph in the mainpage are shown as below:</p>
<ul class="simple">
<li><p>Model Subgraph Name——name of the subgraph.</p></li>
<li><p>Model Subgraph Calculation Load (OPpf)——single frame computing load of the subgraph.</p></li>
<li><p>Model Subgraph DDR Occupation(Mbpf)——single frame loading/writing volume (by MB) of the subgraph.</p></li>
<li><p>Model Subgraph Latency(ms)——single frame computing time consumption of a the subgraph (by millisecond).</p></li>
</ul>
<p>Each subgraph contains a detail page, in which you can get more specific information.
All the above indicators are obtained from the detail page.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note that the detail page varies depending on your specified debugging levels.
In below screenshot, the <strong>Layer Details</strong> can only be obtained when the <code class="docutils literal notranslate"><span class="pre">debug</span></code>
parameter in the configuration file is specified as <code class="docutils literal notranslate"><span class="pre">True</span></code>.
Configuration of the <code class="docutils literal notranslate"><span class="pre">debug</span></code> parameter please refer to the
<a class="reference internal" href="#makertbin"><span class="std std-ref">Convert The Model Using The hb_mapper makertbin Tool</span></a> section.</p>
</div>
<p><strong>Layer Details</strong> analyzes specific OP level information, it is a useful reference in the debugging and analysis stage,
especially when there is low model performance problem caused by BPU OPs, it can help you find the problematic OPs.</p>
<img alt="_images/layer_details.png" class="align-center" src="_images/layer_details.png" />
<p><strong>BIN Model Structure</strong> is a subgraph level visualization results of bin model, in which the nodes in dark color
represent those subgraphs run in BPU; while the nodes in gray represent those nodes computed in CPU.</p>
<p>The purpose of using the <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> is to get to know the subgraph structure of bin model.
In addition, this tool can also provide comprehensive static analysis indicators.
But the <code class="docutils literal notranslate"><span class="pre">hb_perf</span></code> doesn’t contain computing evaluations of the CPU part,
however, this is not a big deal as long as CPU computations only cover those regular processing
at the model input or output parts without those extensive computing nodes. Otherwise you’ll need
to utilize some dev board tool to evaluate the actual model performance.</p>
</section>
<section id="evaluate-actual-model-performance-on-dev-board">
<h3><span class="section-number">3.5.2. </span>Evaluate Actual Model Performance on Dev Board<a class="headerlink" href="#evaluate-actual-model-performance-on-dev-board" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code> tool is used for evaluating actual model performance on dev board.
The <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span></code> is a model execution tool who can evaluate models’ inference performance
and obtain model information. On the one hand, it enables users to get models’ actual performance,
on the other hand, it helps users get to know models’ speed limit and better realize users’ applications
optimization targets.</p>
<p>There are 2 preparatory works before using the <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code> tool:</p>
<ol class="arabic simple">
<li><p>Ensure that you’ve successfully installed the dev board tools according to the instructions in
<a class="reference internal" href="chapter_2_prerequisites.html"><span class="doc">Chapter 2: Prerequisites</span></a>.</p></li>
<li><p>Copy the bin models from your Ubuntu/CentOS dev machine to the dev board (suggested into the <cite>/userdata</cite> directory).
As there is a Linux operating system on the dev board, you can use some popular Linux commands, such as the <code class="docutils literal notranslate"><span class="pre">scp</span></code>
to copy the models.</p></li>
</ol>
<p>Run the <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code> command to evaluate actual model performance.
Refer to below code block:</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note that to run this in dev board.</p>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./hrt_model_exec perf --model_file  mobilenetv1_224x224_nv12.bin <span class="se">\</span>
                      --model_name<span class="o">=</span><span class="s2">&quot;&quot;</span> <span class="se">\</span>
                      --core_id<span class="o">=</span><span class="m">0</span> <span class="se">\</span>
                      --frame_count<span class="o">=</span><span class="m">200</span> <span class="se">\</span>
                      --perf_time<span class="o">=</span><span class="m">0</span> <span class="se">\</span>
                      --thread_num<span class="o">=</span><span class="m">1</span> <span class="se">\</span>
                      --profile_path<span class="o">=</span><span class="s2">&quot;.&quot;</span>
</pre></div>
</div>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></span></dt>
<dd><dl class="simple">
<dt>model_file:</dt><dd><p>refers to the name of the bin model.</p>
</dd>
<dt>model_name:</dt><dd><p>This parameter requires the name of the BIN model to be analyzed.
Note that the name can be omitted when only one model is specified by the <code class="docutils literal notranslate"><span class="pre">model_file</span></code>.</p>
</dd>
<dt>core_id:</dt><dd><p>Default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>. This parameter specifies the core ID to run model.
<code class="docutils literal notranslate"><span class="pre">0</span></code> denotes arbitrary core; <code class="docutils literal notranslate"><span class="pre">1</span></code> donotes core 0; <code class="docutils literal notranslate"><span class="pre">2</span></code> denotes core 1.
You can specify it as <code class="docutils literal notranslate"><span class="pre">0</span></code> when analyzing the limit of both cores.</p>
</dd>
<dt>frame_count:</dt><dd><p>the default value is <code class="docutils literal notranslate"><span class="pre">200</span></code>. This parameter specifies the inference frame number,
the tool will execute the specified number of times then analyze the average time consumption.
It will become valid when the <code class="docutils literal notranslate"><span class="pre">perf_time</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p>
</dd>
<dt>perf_time:</dt><dd><p>Default value is <code class="docutils literal notranslate"><span class="pre">0</span></code>. Measured by minute.
It denotes inference time, the tool will analyze the average time consumption when completing the specified execution time.</p>
</dd>
<dt>thread_num:</dt><dd><p>Default value is <code class="docutils literal notranslate"><span class="pre">1</span></code>. It is used for specifying number of thread and the value range is <code class="docutils literal notranslate"><span class="pre">[1,8]</span></code>.
To analyze limiting frame rate, you need to increase number of thread.</p>
</dd>
<dt>profile_path:</dt><dd><p>By default it is off. It is used for specifying the path to generate log.
Analysis results will be save into the profiler.log file in specified path.</p>
</dd>
</dl>
</dd></dl>

<p>You should see below message at the console after running this command.
As shown below, the <code class="docutils literal notranslate"><span class="pre">Average</span> <span class="pre">latency</span></code> and <code class="docutils literal notranslate"><span class="pre">Frame</span> <span class="pre">rate</span></code> respectively denotes the average single frame inference latency
and the limit frame rate of the model. If you want to get the speed limit of model on dev board, you will need to increase
the <code class="docutils literal notranslate"><span class="pre">thread_num</span></code> till it is good enough.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>Running condition:
Thread number is: <span class="m">1</span>
Frame count   is: <span class="m">200</span>
core number   is: <span class="m">1</span>
Program run time: <span class="m">726</span>.604000  ms
Perf result:
Frame totally latency is: <span class="m">714</span>.537781  ms
Average    latency    is: <span class="m">3</span>.572689  ms
Frame      rate       is: <span class="m">275</span>.253095  FPS
</pre></div>
</div>
<p>There are only overall messages shown at the console, you can control the node_profiler.log file by specifing
the <code class="docutils literal notranslate"><span class="pre">profile_path</span></code> parameter in order to generate more detailed information.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">{</span>
  <span class="s2">&quot;model_latency&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;MOBILENET_subgraph_0&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">2</span>.889,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">2</span>.889,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">2</span>.889
    <span class="o">}</span>,
    <span class="s2">&quot;MOBILENET_subgraph_0_output_layout_convert&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.017265,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.038,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.015
    <span class="o">}</span>,
    <span class="s2">&quot;fc7_1_HzDequantize&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.07467,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.146,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.069
    <span class="o">}</span>,
    <span class="s2">&quot;prob&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.08839,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.172,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.052
    <span class="o">}</span>
  <span class="o">}</span>,
  <span class="s2">&quot;task_latency&quot;</span>: <span class="o">{</span>
    <span class="s2">&quot;TaskRunningTime&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">3</span>.43695,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">5</span>.883,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">3</span>.354
    <span class="o">}</span>,
    <span class="s2">&quot;TaskScheduleTime&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.07456,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.215,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.054
    <span class="o">}</span>,
    <span class="s2">&quot;TaskSubmitTime&quot;</span>: <span class="o">{</span>
      <span class="s2">&quot;avg_time&quot;</span>: <span class="m">0</span>.00861,
      <span class="s2">&quot;max_time&quot;</span>: <span class="m">0</span>.106,
      <span class="s2">&quot;min_time&quot;</span>: <span class="m">0</span>.006
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</div>
<p>The above messages correspond with the bin model visualization of the <strong>BIN Model Structure</strong> section in the
<a class="reference internal" href="#hb-perf"><span class="std std-ref">Use The hb_perf Tool To Evaluate Model Performance</span></a> section and each node corresponds with one node
in the node_profiler.log file by the <code class="docutils literal notranslate"><span class="pre">name</span></code>.
The profiler.log file records the execution time of each node, who is of significance in node optimization.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">profiler</span></code> is a popular operation. As previously described in the
<a class="reference internal" href="#check-result"><span class="std std-ref">Interpret Model Check Results</span></a> section that:
“You don’t need to overly concern with those CPU OPs at the model checking stage”,
at this point, you can find out the specific time consumption of those CPU OPs.
And if you think that the time consumption of CPU OP is prolonged,
now it’s a good time to consider optimizing it.</p>
</section>
<section id="model-performance-optimization">
<span id="id6"></span><h3><span class="section-number">3.5.3. </span>Model Performance Optimization<a class="headerlink" href="#model-performance-optimization" title="Permalink to this headline"></a></h3>
<p>This section deals with the cases when you find out that the model performance in the above analysis results fails your
expectations, it is composed by 4 subsections:</p>
<ol class="arabic simple">
<li><p>Check Those Performance-affecting YAML Configuration Parameters;</p></li>
<li><p>CPU OP Processing;</p></li>
<li><p>High Performance Model Design Proposal;</p></li>
<li><p>Use Horizon Platform Friendly Structure &amp; Model.</p></li>
</ol>
<p>Because some optimizations may influence the parameter space of the original FPM, in other words,
it may cause model retraining. To prevent the costs of repeated adjustments and retraining brought
about by model performance optimization, we suggest that you use random parameters to export models
and validate performance before getting satisfactory model performance.</p>
<section id="check-those-performance-affecting-yaml-configuration-parameters">
<span id="performance-affecting-parameters"></span><h4><span class="section-number">3.5.3.1. </span>Check Those Performance-affecting YAML Configuration Parameters<a class="headerlink" href="#check-those-performance-affecting-yaml-configuration-parameters" title="Permalink to this headline"></a></h4>
<p>Some parameters in the model conversion configuration file can affect model’s final performance, you can check if
they’ve been correctly specified as you expected. definitions and functions of all parameters please refer to the
<a class="reference internal" href="#compiler-parameters"><span class="std std-ref">Compilation Parameters</span></a> table.</p>
<ul>
<li><p>The <code class="docutils literal notranslate"><span class="pre">layer_out_dump</span></code> parameter is used for specifying whether or not to dump the intermediate results in model conversion.
Strictly speaking, this is only a debugging parameter. It will add a dequantize output node to each convolutional operator
and hence significantly reduce model’s onboard performance when specified as <code class="docutils literal notranslate"><span class="pre">True</span></code>. Therefore, please remember to specify it
as <code class="docutils literal notranslate"><span class="pre">False</span></code> in performance evaluation.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">optimize_level</span></code> parameter is recommended to be specified as <code class="docutils literal notranslate"><span class="pre">O3</span></code> when you are going to deliver your product
in order to obtain better onboard performance.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note the spelling of the <code class="docutils literal notranslate"><span class="pre">O3</span></code>, it is the O for owl, not zero.</p>
</div>
</li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">compile_mode</span></code> parameter can be specified as either bandwidth-saving or latency-optimizing.
Remember to adjust it according to your priority.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">core_num</span></code> parameter can reduce single-frame inference latency by increasing number of core;
however the cost of which will be the overall throughput rate.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">debug</span></code> parameter will enable the compiler’s debugging mode when specified as <code class="docutils literal notranslate"><span class="pre">True</span></code> and hence dump simulation
related information, e.g. framerate, DDR bandwidth usage etc. Typically, it is used at the performance evaluation stage.
At the product delivery stage, it can be disabled so as to decrease model size and improve model execution efficiency.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">core_num</span></code> parameter, when specified as <code class="docutils literal notranslate"><span class="pre">2</span></code>, can run models using 2 cores simultaneously so as to reduce the
single-core inference latency. Yet, it can affect the overall throughout rate.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">debug</span></code> parameter, when specified as <code class="docutils literal notranslate"><span class="pre">True</span></code>, will enable the compiler’s debug model so as to dump some
performance simulation related information, e.g. frame rate, DDR bandwidth occupation etc. It is mostly used at the
performance evaluation stage and can be turned off at the product release stage so as to boost models’ execution efficiency.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">max_time_per_fc</span></code> parameter is used for controlling the execution time of compiled binary model directive so as to
implement the model preemption feature on devboard. Changing the execution time of the preempted model by specifying this
parameter can affect the model’s performance on devboard.</p></li>
</ul>
</section>
<section id="cpu-op-processing">
<h4><span class="section-number">3.5.3.2. </span>CPU OP Processing<a class="headerlink" href="#cpu-op-processing" title="Permalink to this headline"></a></h4>
<p>When it is confirmed by the <code class="docutils literal notranslate"><span class="pre">hrt_model_exec</span> <span class="pre">perf</span></code>’s evaluations that the performance bottleneck lies in CPU OPs,
we suggest that you should confirm if the OPs which currently running in CPU can be supported by the BPU as described in
the <strong>supported_op_list_and_restrictions</strong> Excel file.</p>
<p>If the OP(s) can be supported by the BPU, then it must be the case that your OP parameters have exceeded the BPU supported
parameter restrictions. In such case, just adjust the corresponding computing parameters of the original FPM back into the
restricted range. To help you quickly find out the off-limits parameter(s), we suggest that you proceed the model check
procedure as described in the <a class="reference internal" href="#model-check"><span class="std std-ref">Check The Model</span></a> section, the tool will print out the off-limits parameters
at the console.</p>
<p>Note that you’ll need to handle the effect on model performance (if any) caused by modifying the original FPM parameters.
Take the <code class="docutils literal notranslate"><span class="pre">input_channel</span></code> or <code class="docutils literal notranslate"><span class="pre">output_channel</span></code> of Convolution exceeding restrictions as classic examples, by reducing
number of channels to quickly enable the OP to be supported by the BPU can also affect model accuracy.</p>
<p>If the OP cannot be supported by the BPU, you will need to find out a substitution among those BPU supported OPs and
replace the BPU unsupported OP with the substitution in the original FPM. Another example takes place in computing
extensive OPs. Horizon’s BPU can support most of them and has optimized those OPs who can only run in CPU, but multiple
subgraphs in the bin model can still happen when there are repeatedly-used BPU unsupported activation function(s).</p>
</section>
<section id="high-performance-model-design-proposal">
<h4><span class="section-number">3.5.3.3. </span>High Performance Model Design Proposal<a class="headerlink" href="#high-performance-model-design-proposal" title="Permalink to this headline"></a></h4>
<p>Based on the performance evaluation results, you can see that the CPU time consumption proportion is actually very small
and that the major performance bottleneck lies in the prolonged BPU inference time consumption. In such cases, as we’ve
already utilized all computing components, the remaining optimization space lies in the use ratio improvement of
computing resources. As each AI ASIC has its own hardware characteristics, whether the computing parameters of algorithm
models fit hardware characteristics well can directly determine the use ratio of computing resources. The better they fit,
the higher the use ratio and vice versa. This section elaborates the hardware characteristics of Horizon’s hardwares.</p>
<p>Firstly, Horizon’s AI ASICs are designed for CNN (Convolution Neural Network) acceleration and the major computing
resources are made for processing all types of convolutions. Therefore, your models are expected to be convolution-oriented,
any other types of OPs can lead to reduced computing resource utilization and the impact varies depending on different OPs.</p>
<p>🛠️ <strong>Overall Hardware Requirements</strong></p>
<p>Below table lists some hardware level computing-friendly requirements for your reference.</p>
<table class="docutils align-center">
<colgroup>
<col style="width: 12%" />
<col style="width: 38%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>OPERATOR</p></td>
<td><p>RESTRICTION</p></td>
<td><p>NOTE</p></td>
</tr>
<tr class="row-even"><td rowspan="5"><p>Convolution</p></td>
<td><p>Kernel HxW=[1,7]x[1,7]</p></td>
<td><p>It can cause waste of computing power when the kernel size equals 2, 4 or 6</p></td>
</tr>
<tr class="row-odd"><td><p>Channel Num (one group) &lt;= 2048</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Conv with sumin: Stride∈{1, 2}, Others: no restriction</p></td>
<td><p>It can cause waste of computing power when the Stride &gt; 2.</p>
<p>It can lead to additional padding operations when the Pad doesn’t equal kernel_size/2 and hence cause reduced model performance.</p>
</td>
</tr>
<tr class="row-odd"><td><p>Dilation value must be divisible by stride</p></td>
<td><p>Dilation can lead to additional data migration.</p></td>
</tr>
<tr class="row-even"><td><p>Size of Kernel: HxWxC &lt;= 32768</p></td>
<td></td>
</tr>
<tr class="row-odd"><td rowspan="4"><p>Deconvolution</p></td>
<td><p>Kernel HxW=[2,14]x[2,14]</p></td>
<td rowspan="4"><p>Deconvolution is not natively supported by BPU.</p></td>
</tr>
<tr class="row-even"><td><p>Channel Num &lt;= 2048</p></td>
</tr>
<tr class="row-odd"><td><p>Padding HxW=[0,(Kernel_H-1)/2]x[0,(Kernel_W-1)/2]</p></td>
</tr>
<tr class="row-even"><td><p>Stride ∈ {2, 4}</p></td>
</tr>
<tr class="row-odd"><td rowspan="4"><p>Fully Connected Convolution</p></td>
<td><p>Kernel HxW=[1,31]x[1,31], and HxW &lt;= 127</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Channel Num∈[1,2048], or &lt;= 16384 if H and W are both 1</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>for int8 output: HxCEIL(W/8)xCEIL(C/4) &lt;= {512(X2/J2), 1024(X3J3)}</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>for int32 output: HxCEIL(W/8)xCEIL(C/4) &lt; {1024(X2/J2), 2048(X3J3)}</p></td>
<td></td>
</tr>
<tr class="row-odd"><td rowspan="4"><p>Pooling</p></td>
<td><p>Average pooling: Kernel HxW=[1,7]x[1,7], Stride∈{1, 2}, Padding HxW=[0,Kernel_H/2]x[0,Kernel_W/2]</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Global average pooling: Kernel HxW &lt;= 8192</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Max pooling: Kernel HxW=[1, 64]x[1,64], Stride=[1,256], Padding &gt;= 0</p></td>
<td><p>There can be additional costs when Padding &gt; 1 and Stride &gt; 2.</p></td>
</tr>
<tr class="row-even"><td><p>Global max pooling: Kernel HxW=[1,1024]x[1,1024]</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Upscale</p></td>
<td><p>Scaling proportional range (1/256,256], precision=1/256</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>RoiAlign/Roiresize</p></td>
<td><p>Scaling proportional range (1/256,256], precision=1/256</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Channel Concat</p></td>
<td><p>None</p></td>
<td><p>Time consumption will increase when the channel num of the Input feature is not multiples of 4.</p></td>
</tr>
<tr class="row-even"><td><p>Channel Split</p></td>
<td><p>Input feature channel is multiple of split number.</p></td>
<td><p>Time consumption will increase when the channel num of the Output features is not multiples of 4.</p></td>
</tr>
<tr class="row-odd"><td><p>Slice</p></td>
<td><p>None</p></td>
<td><p>Time consumption will increase when the W in the starting coordinates is not multiples of 8.</p>
<p>The slice alongside channel direction can occupy MAC computing resources.</p>
</td>
</tr>
<tr class="row-even"><td><p>Upsample</p></td>
<td><p>mode={nearest}, HxWxC -&gt; (2H)x(2W)xC</p></td>
<td></td>
</tr>
<tr class="row-odd"><td rowspan="3"><p>Reshape</p></td>
<td><p>Reshape in the H and W directions, currently N and C are not supported.</p></td>
<td><p>Time consumption can be mass when the W of the Input/Output feature is not multiples of 8.</p></td>
</tr>
<tr class="row-even"><td><p>reorder upscale: HxWxC -&gt; (2H)x(2W)x(C/4)</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>stack neighbor: HxWxC -&gt; (H/2)x(W/2)x(4C)</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Shuffle</p></td>
<td><p>Input feature channel &lt;= 2048, only supports shuffle in C direction</p></td>
<td><p>It can occupy MAC computing resources if the granularity of shuffle is not multiples of 4.</p></td>
</tr>
<tr class="row-odd"><td><p>Elementwise Add</p></td>
<td><p>Input feature channel &lt;= 2048</p></td>
<td><p>It can occupy MAC computing resources.</p></td>
</tr>
<tr class="row-even"><td><p>Elementwise Mul</p></td>
<td><p>Input feature channel &lt;= 2048</p></td>
<td><p>It can occupy MAC computing resources and has lower efficiency.</p></td>
</tr>
<tr class="row-odd"><td><p>Broadcast Mul</p></td>
<td><p>Input feature channel &lt;= 2048</p></td>
<td><p>It can occupy MAC computing resources and has lower efficiency.</p></td>
</tr>
<tr class="row-even"><td><p>Elementwise Max/Min</p></td>
<td><p>Input feature channel &lt;= 2048</p></td>
<td><p>It can occupy MAC computing resources and has lower efficiency.</p></td>
</tr>
<tr class="row-odd"><td><p>LookupTable (sigmoid,tanh..)</p></td>
<td><p>Lookup table: int8 -&gt; int8</p></td>
<td><p>It can occupy MAC computing resources and has lower efficiency.</p></td>
</tr>
<tr class="row-even"><td><p>Pad</p></td>
<td><p>Pad Zero, Constant or Boundary</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Cross Channel Max</p></td>
<td><p>Input feature channel ∈ [1, 64*group_num].</p></td>
<td></td>
</tr>
<tr class="row-even"><td rowspan="3"><p>Detection Post Process</p></td>
<td><p>Filter + Sort + NMS</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Anchor num: [1, 64], Class num: [1, 64]</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Max output num: 4096</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Leaky Relu</p></td>
<td><p>None</p></td>
<td><p>It can occupy MAC computing resources and has lower efficiency.</p></td>
</tr>
<tr class="row-even"><td><p>Prelu</p></td>
<td><p>None</p></td>
<td><p>It can occupy MAC computing resources and has lower efficiency.</p></td>
</tr>
<tr class="row-odd"><td><p>Relu/Relu6</p></td>
<td><p>None</p></td>
<td><p>It can occupy MAC computing resources and has lower efficiency.</p></td>
</tr>
</tbody>
</table>
<p><strong>Convolution Width Alignment</strong></p>
<p>Due to the alignment requirement when computing MAC array, it can increase the efficiency when W of featuremap is
multiples of 8 (when Convolution stride = 2, W should be multiples of 16).
There can cause waste of computing power and lead to reduced MAC utilization when the above-mentioned W alignments don’t
satisfy the multiples of 8 or 16 requirements.
For example, when the input feature size of convolution is 1x8x9x32 (NHWC), in actual computations, the W will be padded
as 15 (i.e. the feature size becomes 1x8x16x32) and will cause waste of computing resources.</p>
<p>When designing the network, the MAC utilization will be directly increased if the input size (align upwards or downwards)
of the entire neural network can be changed.</p>
<p>Below is an sample in which displays the differences of model input sizes equal 224 or 256/192 when
there is a multi-layered network (based on ResNet) whose stride=2.</p>
<img alt="_images/width_alignment.png" class="align-center" src="_images/width_alignment.png" />
<p><strong>Convolution Channel Alignment</strong></p>
<p>As the Channel must be multiples of 8 in hardware, it is better to adjust the kernel num to multiples of 8 when
designing the algorithm.</p>
<img alt="_images/channel_alignment.png" class="align-center" src="_images/channel_alignment.png" />
<p>The alignment of the channel of Group Convolution is more complicated.</p>
<img alt="_images/group_channel_alignment.png" class="align-center" src="_images/group_channel_alignment.png" />
<p>If the Kernel doesn’t equal integer multiples of 8, then the kernel num of each group need to be multiples of 8.
Also, the alignment here can cause waste of computing power in subsequent convolution.
As shown in the above frame digram, after padding the weight in Convolution2, the other weight in the next layer
will also require padding.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The padding method is to align each group with multiples of 8,
i.e., padding data spread over weight.</p>
</div>
<img alt="_images/group_channel_alignment_2.png" class="align-center" src="_images/group_channel_alignment_2.png" />
<p>If the channels in group is not multiples of 8, then the convolutions of the upper layer must proceed padding.
As shown in the above frame diagram, Convolution1’s kernel num is modified from 48 into 64 by padding.</p>
<p>In addition, there can be greater impact when multiple consecutive unaligned kernel number or channel number take place
in the groups of group convolution. In such cases, we’ll need to consider the alignment requirements of multi-layer group
conv at the same time and can cause more padding. At worst, the group convolution will be converted into ordinary convolution.</p>
<p><strong>Activation Function</strong></p>
<p>Most activation functions require to use LUT and Element-wise OP implementations, although LUT and element-wise operations
can be supported now, they are all combinations of other OPs whose efficiency are too high.</p>
<p>In case there are only a few places in the model where you need to use some activation functions (non-relu) who are not
supported by the hardware but have less computation costs, they won’t affect the computing efficiency of the entire model
on a large scale.</p>
<p>However, when there are a large volume of activation functions who are not supported by the hardware in the model,
they can have great impacts on the model’s execution speed.</p>
<p><strong>Other Suggestions</strong></p>
<p>As the computing efficiency of the depthwise convolution in Horizon’s ASICs can approach 100%, the BPU chips have
efficiency advantage on the MobileNet models.</p>
<p>In addition, when designing the models, the time consumption and hardware’s bandwidth pressure caused by those
quantization and dequantization nodes can be relieved by reducing the input/output dimensions at the models’ BPU segments.
Take the classic segmentation models as an example, we can directly merge the Argmax operator into the model. But note that
the Argmax operator can support BPU acceleration only when the following conditions are met:</p>
<ol class="arabic simple">
<li><p>By default, the axis of the Softmax layer in Caffe equals 1, yet the the axis of ArgMax layer equals 0 by default,
so please keep the axis value the same when replacing the operator.</p></li>
<li><p>The Channel of Argmax should be ≤ 64, otherwise it can only be computed in the CPU.</p></li>
</ol>
</section>
<section id="bpu-high-efficiency-model-optimization">
<h4><span class="section-number">3.5.3.4. </span>BPU High-efficiency Model Optimization<a class="headerlink" href="#bpu-high-efficiency-model-optimization" title="Permalink to this headline"></a></h4>
<p>The academic circle is constantly optimizing the computation efficiency
(under the same algorithm accuracy, the smaller the computation, the higher efficiency) and parameter efficiency
(under the same algorithm accuracy, the less the parameter volume, the higher efficiency) of algorithm models.
The representative works, such as the EfficientNet and the ResNext, have utilized the Depthwise Convolution and the
Group Convolution respectively. However, as the supporting efficiencies of GPU and TPU are very low and cannot make full
use of algorithms’ advantages when confronting such high-efficient models, hence the academic circle was forced to
optimize the EfficientNetV2 and the NFNet for GPU/TPU. The optimizations primarily lie in less use of Depthwise
Convolution and significantly expanding the Group size in Group Convolution. As a results, these modifications have
reduced the computation and parameter efficiencies of the original models.</p>
<p>Horizon’s X3-BPU and J3-BPU have made specific optimizations for Depthwise Convolution and Group Convolution, making it
possible for users to obtain higher computation and parameter efficiencies.</p>
<p>As reference samples of the 2 models, AI Toolchain’s model_zoo release package proudly provides:</p>
<ul class="simple">
<li><p>The efficientnet[-lite] series with a quest for superb computation and parameter efficiencies.
The X3-BPU can provide efficiently support: take the EfficientNet Lite0 as example, the X3-BPU’s
frame rate is 6x higher than some 30TOPS GPU.</p></li>
<li><p>The VarGNet series, Horizon’s own-developed models who take full advantages of Group Convolution’s high efficiency
and optimized the X3-BPU and J3-BPU at the same time. The VarGNet series have been widely applied to Horizon’s
application scenarios. They are of high robustness in terms of training parameters and can be applied to different
tasks with lower hyperparameter tuning costs.</p></li>
</ul>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>There are more model structures and business models underway as we’re constantly working to deliver more.
New models will be contributed to <a class="reference external" href="https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master">https://github.com/HorizonRobotics-Platform/ModelZoo/tree/master</a>.
If you expect more than what we have at hand, welcome to visit Horizon’s only official technical community
(<a class="reference external" href="https://developer.horizon.ai">https://developer.horizon.ai</a>) and post us your ideas.
We’re previleged to offer more constructive advices based on your kind feedbacks.</p>
</div>
</section>
</section>
</section>
<section id="model-accuracy-analysis-and-optimization">
<span id="accuracy-evaluation"></span><h2><span class="section-number">3.6. </span>Model Accuracy Analysis And Optimization<a class="headerlink" href="#model-accuracy-analysis-and-optimization" title="Permalink to this headline"></a></h2>
<p>There are inevitable accuracy loss with the post-training model quantization that converting the floating-point models
into the fixed-point models based on dosens or hundreds of calibration data. But it has been proofed by a large number of
production experience that as long as the most optimized parameter combination can be found out, in most cases, Horizon’s
conversion tools can keep the accuracy loss within 1%.</p>
<p>This section explains how to correctly analyze model accuracy. In case the evaluation results fail your expectations,
please refer to the <a class="reference internal" href="#accuracy-optimization"><span class="std std-ref">Accuracy Optimization</span></a> section and try to optimize the accuracy.
If you still find the problem unsolvable, please don’t hesitate to contact Horizon and seek for technical support.</p>
<section id="model-accuracy-analysis">
<h3><span class="section-number">3.6.1. </span>Model Accuracy Analysis<a class="headerlink" href="#model-accuracy-analysis" title="Permalink to this headline"></a></h3>
<p>You are expected to understand how to evaluate model accuracy when reading this section.
This section explains how to run model inference using the outputs of model conversion.
As previously described, successful model conversion consist of the following 4 outputs:</p>
<ul class="simple">
<li><p>***_original_float_model.onnx</p></li>
<li><p>***_optimized_float_model.onnx</p></li>
<li><p>***_quantized_model.onnx</p></li>
<li><p>***.bin</p></li>
</ul>
<p>Although the bin model is the final input of the AI ASICs, for your easier accuracy evaluations in Ubuntu/CentOS dev machines,
the ***_quantized_model.onnx, who has completed the quantization process and has the same accuracy results as the
bin model is provided. Below code sample displays the basic process to load ONNX inference model using Horizon’s dev libs,
not only does it applies to the quantized model, it also can apply to the original and the optimized models, you only need
to prepare corresponding data in line with different input types and layouts.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># import Horizon&#39;s dependencies</span>
<span class="kn">from</span> <span class="nn">horizon_tc_ui</span> <span class="kn">import</span> <span class="n">HB_ONNXRuntime</span>

<span class="c1"># prepare the feed_dict to run model</span>
<span class="k">def</span> <span class="nf">prepare_input_dict</span><span class="p">(</span><span class="n">input_names</span><span class="p">):</span>
  <span class="n">feed_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">input_name</span> <span class="ow">in</span> <span class="n">input_names</span><span class="p">:</span>
      <span class="c1"># the your_custom_data_prepare denotes your customized data</span>
      <span class="c1"># prepare your data in line with the type and layout requirements of input nodes</span>
      <span class="n">feed_dict</span><span class="p">[</span><span class="n">input_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">your_custom_data_prepare</span><span class="p">(</span><span class="n">input_name</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">feed_dict</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
  <span class="c1"># create inference Session</span>
  <span class="n">sess</span> <span class="o">=</span> <span class="n">HB_ONNXRuntime</span><span class="p">(</span><span class="n">model_file</span><span class="o">=</span><span class="s1">&#39;***_quantized_model.onnx&#39;</span><span class="p">)</span>

  <span class="c1"># obtain input/output node names</span>
  <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="nb">input</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">sess</span><span class="o">.</span><span class="n">get_inputs</span><span class="p">()]</span>
  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="n">output</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">output</span> <span class="ow">in</span> <span class="n">sess</span><span class="o">.</span><span class="n">get_outputs</span><span class="p">()]</span>

  <span class="c1"># prepare model input data</span>
  <span class="n">feed_dict</span> <span class="o">=</span> <span class="n">prepare_input_dict</span><span class="p">(</span><span class="n">input_names</span><span class="p">)</span>
  <span class="c1"># begin model inference, the return is a list corresponding with the output_name specified names</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">output_names</span><span class="p">,</span> <span class="n">feed_dict</span><span class="p">,</span> <span class="n">input_offset</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
<p>As shown in the above sample, the <code class="docutils literal notranslate"><span class="pre">input_offset</span></code> parameter has default value of 128. For model with no preprocess nodes, <code class="docutils literal notranslate"><span class="pre">input_offset</span></code> should be set to 0.</p>
<p>Also, the <code class="docutils literal notranslate"><span class="pre">your_custom_data_prepare</span></code> represented input data preparation is the most likely mistaken
part. Compared with the accuracy validation secting during the original floating-point model design and training, you are
expected to further adjust the inference input data after data pre-processing, especially data format (RGB, NV12 etc.),
accuracy (INT8, FLOAT32 etc.) and layout (NCHW or NHWC). How to specifically adjust the input data depends jointly on your
specified <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code>, <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code>, <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> and <code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code> when converting the model.
Parameter configuration details please refer to the <a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a> section.</p>
<p>For example, there is a ImageNet trained original floating-point model for classification who contains only one input node.
The input node can accept tri-channel image with BGR sequence and input data layout is NCHW.
At the original floating-point model design and training stage, the data pre-processing prior to validation dataset
inference is shown as below:</p>
<ol class="arabic simple">
<li><p>Uniformly scale the image and resize the short side int0 256.</p></li>
<li><p>Obtain 224x224 image using the <code class="docutils literal notranslate"><span class="pre">center_crop</span></code> method.</p></li>
<li><p>Subtract mean value by the channel.</p></li>
<li><p>Multiply scale ratio.</p></li>
</ol>
<p>When converting this original floating-point model using Horizon’s conversion tools,
specify the <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> as <code class="docutils literal notranslate"><span class="pre">bgr</span></code>, <code class="docutils literal notranslate"><span class="pre">input_layout_train</span></code> as <code class="docutils literal notranslate"><span class="pre">NCHW</span></code>,
<code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> as <code class="docutils literal notranslate"><span class="pre">bgr</span></code> and <code class="docutils literal notranslate"><span class="pre">input_layout_rt</span></code> as <code class="docutils literal notranslate"><span class="pre">NHWC</span></code>.
According to the rules described in the <a class="reference internal" href="#conversion-interpretation"><span class="std std-ref">Model Conversion Interpretation</span></a>,
the ***_quantized_model.onnx accepts bgr_128 with NHWC layout.
In correspondence with the above-mentioned sample, the <code class="docutils literal notranslate"><span class="pre">your_custom_data_prepare</span></code> part of pre-processing
should be the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># this sample uses the skimage library and there are differences when using the opencv library</span>
<span class="c1"># please note that the mean subtraction and scale multiplication operations is not shown in the transformers</span>
<span class="c1"># the mean and scale operations have been fused into the model,</span>
<span class="c1"># refer to the previous norm_type/mean_values/scale_values configurations</span>
<span class="k">def</span> <span class="nf">your_custom_data_prepare_sample</span><span class="p">(</span><span class="n">image_file</span><span class="p">):</span>
  <span class="c1"># When reading images using the skimage library, the layout is NHWC layout already</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">skimage</span><span class="o">.</span><span class="n">img_as_float</span><span class="p">(</span><span class="n">skimage</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">image_file</span><span class="p">))</span>
  <span class="c1"># uniformly scale the images and resize the short side to 256</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">ShortSideResize</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">short_size</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
  <span class="c1"># obtain 224x224 images using the CenterCrop</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">CenterCrop</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">crop_size</span><span class="o">=</span><span class="mi">224</span><span class="p">)</span>
  <span class="c1"># the channel sequency is RGB whening reading results using the skimage,</span>
  <span class="c1"># then convert into the bgr_128 required BGR sequence</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">RGB2BGR</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
  <span class="c1"># As number reading range of skimage is [0.0,1.0], adjust to the bgr required number range</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">*</span> <span class="mi">255</span>
  <span class="c1"># the bgr_128 subtracts 128 from bgr</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">-</span> <span class="mi">128</span>
  <span class="c1"># bgr_128 uses int8</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
  <span class="c1"># expend N dimension, data layout is NHWC</span>
  <span class="c1"># if multiple batches are used, then splice multiple data together</span>
  <span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">image</span>
</pre></div>
</div>
</section>
<section id="accuracy-optimization">
<span id="id7"></span><h3><span class="section-number">3.6.2. </span>Accuracy Optimization<a class="headerlink" href="#accuracy-optimization" title="Permalink to this headline"></a></h3>
<p>Based on previous accuracy analysis results, the accuracy loss problem of quantized model can be divided into 2 below
types:</p>
<ul class="simple">
<li><p>There are apparent accuracy losses (over 4%).
This can be mostly caused by either inappropriate yaml configurations or unbalanced calibration datasets etc.,
Please troubleshoot according to below advices.</p></li>
<li><p>Accuracy loss is small (1.5%~3%).
If there are still small accuracy losses after the above cause is excluded, it is usually caused by model sensitivity
and can be optimized using our accuracy optimization tool.</p></li>
</ul>
<p>The workflow chart of accuracy loss solution is shown as below:</p>
<a class="reference internal image-reference" href="_images/accuracy_problem.png"><img alt="_images/accuracy_problem.png" class="align-center" src="_images/accuracy_problem.png" style="width: 761.6px; height: 1272.0px;" /></a>
<section id="apparent-accuracy-loss-over-4">
<h4><span class="section-number">3.6.2.1. </span>Apparent Accuracy Loss (Over 4%)<a class="headerlink" href="#apparent-accuracy-loss-over-4" title="Permalink to this headline"></a></h4>
<p>Apparent accuracy loss are usually caused by all types of improper configurations, therefore, we suggest that you
doublecheck the pipeline, model conversion configurations and consistency.</p>
<p><strong>Doublecheck The Pipeline</strong></p>
<p>Pipeline refers to the entire process of data preparations, inference, post-processing and the accuracy evaluation Metric.
Based on the past customer problem follow-up experiences, we find out that the most commonly seen case is that the
modifications in the FPM training stage are not updated timely to the accuracy validation during model conversion stage.</p>
<p><strong>Doublecheck The Model Conversion Configurations</strong></p>
<ul class="simple">
<li><p>As the <code class="docutils literal notranslate"><span class="pre">input_type_rt</span></code> and <code class="docutils literal notranslate"><span class="pre">input_type_train</span></code> parameters are used for distinguishing the data formats of the converted
HGM and the original FPM, it must be carefully doublechecked if they can satisfy the expectation, especially the sequences
of BGR and RGB channels.</p></li>
<li><p>Doublecheck if the <code class="docutils literal notranslate"><span class="pre">norm_type</span></code>, <code class="docutils literal notranslate"><span class="pre">mean_values</span></code> and <code class="docutils literal notranslate"><span class="pre">scale_values</span></code> parameters are specified correctly.
Nodes of the mean and scale operations can be directly inserted into the model by specifying the conversion
configurations, and it should be confirmed whether repeated mean or scale operations are executed in the
validation/evaluation images. Repeated pre-processing operation is another frequently-seen mistake.</p></li>
<li><p>Doublecheck if the <code class="docutils literal notranslate"><span class="pre">preprocess_on</span></code> is ON as it determines whether or not to resize or transfer the colors of
calibration images. It is recommended to turn off this parameter.</p></li>
</ul>
<p><strong>Doublecheck Data Processing Consistency</strong></p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">skimage.read</span></code> and <code class="docutils literal notranslate"><span class="pre">opencv.imread</span></code> are 2 popular image-reading methods, while there are differences in the
output ranges and formats between the 2 methods. When using the <code class="docutils literal notranslate"><span class="pre">skimage</span></code> to read images, you can get RGB channel
sequence, value ranges between 0~1 and float data type; but when using the <code class="docutils literal notranslate"><span class="pre">opencv</span></code>, you will get BGR channel
sequence, value ranges between 0~225 and uint8 data type.</p></li>
<li><p>At the calibration data preparation stage, we often use numpy’s tofile serialized data when preparing application
samples for applications. As this method doesn’t save the shape and type information, we need to manually specify them
when loading in order to ensure the data type, size and layout etc. consistencies in serialization and deserialization.</p></li>
<li><p>When using Horizon’s AI Toolchain, you’re recommended to use the same data processing libraries as in the FPM training
validation stage, in order to maintain model accuracy of some models with low robustness. Because the different
implementations of resize, crop etc. operations of different libraries may affect model accuracy.</p></li>
<li><p>Validate if datasets are reasonably distributed. The volume of validation dataset should be around 100 and images should
cover all scenarios. For example, in cases of multi-task and multi-class classification, the validation dataset should be
able to cover all prediction branches or all classes. Meanwhile, try not to use those exceptional images
(e.g. the over-exposed).</p></li>
<li><p>Use the ***_original_float_model.onnx model to re-validate model accuracy. Normally, the accuracy of the
***_original_float_model.onnx should be accurate to 3~5 decimal places. If your model fails to satisfy this accuracy,
please carefully check the data processing.</p></li>
</ul>
</section>
<section id="improve-model-accuracy-when-there-is-smaller-accuracy-loss">
<h4><span class="section-number">3.6.2.2. </span>Improve Model Accuracy When There Is Smaller Accuracy Loss<a class="headerlink" href="#improve-model-accuracy-when-there-is-smaller-accuracy-loss" title="Permalink to this headline"></a></h4>
<p>Normally to make it easier to optimize model accuracy, you’re recommended to use the automatic parameter search feature in
conversion configuration. In case you find that the model accuracy based on the automatic search fails the expectations,
and that the range of accuracy loss is within 1.5% ~ 3%, try to use below tips to improve model accuracy.</p>
<ul class="simple">
<li><p>Try to manually specify the <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code>, select either <code class="docutils literal notranslate"><span class="pre">kl</span></code> or <code class="docutils literal notranslate"><span class="pre">max</span></code>.</p></li>
<li><p>Try to enable the <code class="docutils literal notranslate"><span class="pre">per_channel</span></code> parameter.</p></li>
<li><p>When the <code class="docutils literal notranslate"><span class="pre">calibration_type</span></code> is specified as <code class="docutils literal notranslate"><span class="pre">max</span></code>, try to specify the <code class="docutils literal notranslate"><span class="pre">max_percentile</span></code> into
<code class="docutils literal notranslate"><span class="pre">0.99999</span></code>, <code class="docutils literal notranslate"><span class="pre">0.99995</span></code>, <code class="docutils literal notranslate"><span class="pre">0.9999</span></code>, <code class="docutils literal notranslate"><span class="pre">0.9995</span></code> and <code class="docutils literal notranslate"><span class="pre">0.999</span></code> respectively.</p></li>
</ul>
<p>Based on past production experience, the abovementioned strategies can handle all types of problems.
If you still find the problem unsolved, welcome to post your problem at Horizon’s official technical community
(<a class="reference external" href="https://developer.horizon.ai">https://developer.horizon.ai</a>), so that we can give more specific suggestions.</p>
</section>
</section>
<section id="further-improve-model-accuracy-using-the-qat-solution">
<h3><span class="section-number">3.6.3. </span>Further Improve Model Accuracy Using The QAT Solution<a class="headerlink" href="#further-improve-model-accuracy-using-the-qat-solution" title="Permalink to this headline"></a></h3>
<p>If you’ve tried out the above-mentioned method, and there’s nothing wrong with the configurations,
but the model accuracy still fails your expectation, then it could be the inherent limitations of the PTQ (Post-training Quantization) solution
that caused the accuracy issue. In such case, you can utilize the QAT (Quantization Aware Training) Solution to quantize the model.</p>
<p>This section elaborates the QAT Solution as follows:</p>
<ul class="simple">
<li><p>Firstly, the <a class="reference internal" href="#about-quantization"><span class="std std-ref">About Quantization</span></a> subsection introducess the concept and 2 different methods of quantization.</p></li>
<li><p>Secondly, the <a class="reference internal" href="#about-conversion"><span class="std std-ref">About Model Conversion</span></a> subsection tells you what is Horizon’s model conversion all about,
what is the oringial floating-point model and what is a heterogeneous model.</p></li>
<li><p>Thirdly, based on the understanding of the above-mentioned concepts,
the <a class="reference internal" href="#about-quantization-compile"><span class="std std-ref">About Model Quantization &amp; Compilation</span></a>
subsection tells you more about the relationship between PTQ and QAT,
so that you can choose an appropriate solution based on your own conditions;</p></li>
<li><p>Lastly, the <a class="reference internal" href="#qat-quantzation-compile"><span class="std std-ref">QAT Model Quantization &amp; Compilation</span></a> subsection further elaborates how to accomplish the model
compilation of the quantized model in conjunction with PyTorch’s QAT solution.
This subsection also introduces the <strong>APIs</strong> and <strong>A Complete Quantization Sample</strong> for developers.</p></li>
</ul>
<section id="about-quantization">
<span id="id8"></span><h4><span class="section-number">3.6.3.1. </span>About Quantization<a class="headerlink" href="#about-quantization" title="Permalink to this headline"></a></h4>
<p>Currently most GPU trained models are using floating-point number representation, i.e., model parameters are stored using floating-point numbers.
Horizon’s AI ASICs based on the BPU architecture are using the INT8 number (a common precision in the industry) representation,
they can support quantized model using the fixed-point number representations.
The very process of <strong>converting the model using floating-point parameters into the model using fixed-point parameters</strong> is what we call quantization.</p>
<p><strong>There are 2 quantization methods:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>PTQ (Post Training Quantization):</strong>:
Firstly train a floating-point model, then calculate the quantization parameters using calibration images,
and lastly convert the floating-point model into quantized model.
This method is easier and faster, but there must be inevitable quantization loss.
The PTQ tools in Horizon’s Model Conversion Toolchain can guarantee that the accuracy loss of over 80% (an estimated value) model is within 1%.</p></li>
</ol>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>More information about the quantization and compilation processes using the PTQ solution please refer to
the above-mentioned contents in this chapter.</p>
</div>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p><strong>QAT (Quantization Aware Training):</strong>
The QAT solution intervenes the structure of the floating-point model during floating-point model training stage,
increasing quantzation error in order to enable the model to aware the loss that will be brought about by quantization.
The QAT solution requires users to retrain the model based on full training set and
can effectively reduce the quantization error in quantization and deployment.
The QAT solution is a popular solution among many open source frameworks, e.g. The Eager Mode and FX Graph solutions of PyTorch,
the tf-lite solution etc.</p></li>
</ol>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p><strong>What is the relationship between QAT and floating-point training</strong></p>
<p>As QAT is actually a finetune method, it is better to use the QAT solution to improve quantization accuracy
after the floating-point parameters are already fitting.
That said, the training consists in 2 steps: firstly, train the floating-point model, till you’re satisfied with the model accuracy,
then use QAT to further improve the quantization accuracy.</p>
<p>To enable the model to better aware the quantzation error, QAT are required to use the full volume training set.
The number of epoch is related to the difficulty level of your model,
approximately the epoch number should equal to 1/10 of the original floating-point traing.
As the finetune is based on the floating-point model,
try to specify the QAT learning rate similar with that of in the last a few epochs of the floating-point model.</p>
</div>
</div></blockquote>
</div></blockquote>
</section>
<section id="about-model-conversion">
<span id="about-conversion"></span><h4><span class="section-number">3.6.3.2. </span>About Model Conversion<a class="headerlink" href="#about-model-conversion" title="Permalink to this headline"></a></h4>
<p>Model conversion refers to the very process of converting the original floating-point model into Horizon supported heterogeneous model.
The process consists of pre-process node modification, original model graph optimization, model quantization and model compilation in devboard etc.</p>
<p><strong>The floating-point Model</strong> (partially referred to as the floating-point model) refers to the models users obtained from open source frameworks,
such as: TensorFlow/PyTorch etc. The parameters of these models are stored using floating-point numbers.
Currently our QAT solution is based on PyTorch’s QAT solution, therefore it can only support PyTorch models.
The PTQ solution can only support Caffe and ONNX models, so users need to firstly convert the TensorFlow/PyTorch models into ONNX models before
using Horizon’s PTQ model conversion toolchain to quantize and compile models.</p>
<p><strong>The heterogeneous model</strong> refers to such a model format that is applicable to Horizon’s ASIC.
It is called heterogeneous model because it can support both ARM CPU and BPU executions.
Because the execution speed in the BPU is far more superior to that of in the CPU, please try to calculate your operators in the BPU.
Those BPU unapplicable operators will be executed by the CPU.</p>
</section>
<section id="about-model-quantization-compilation-workflow">
<span id="about-quantization-compile"></span><h4><span class="section-number">3.6.3.3. </span>About Model Quantization &amp; Compilation Workflow<a class="headerlink" href="#about-model-quantization-compilation-workflow" title="Permalink to this headline"></a></h4>
<p>Below displays a normal workflow of model quantization and compilation:</p>
<figure class="align-center" id="id9">
<a class="reference internal image-reference" href="_images/qat_compile_flow.png"><img alt="_images/qat_compile_flow.png" src="_images/qat_compile_flow.png" style="width: 1324.5px; height: 292.0px;" /></a>
<figcaption>
<p><span class="caption-text">Single click to enlarge the image</span><a class="headerlink" href="#id9" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Because the cost of PTQ solution is smaller, users are recommended to quantize and compile the model firstly using the PTQ solution.
In case the model accuracy cannot satisfy your requirement after PTQ and optimization, please try out the QAT solution instead.</p>
</div>
</section>
<section id="qat-model-quantization-compilation">
<span id="qat-quantzation-compile"></span><h4><span class="section-number">3.6.3.4. </span>QAT Model Quantization &amp; Compilation<a class="headerlink" href="#qat-model-quantization-compilation" title="Permalink to this headline"></a></h4>
<p><strong>PyTorch’s QAT Solution (The FX Graph Quantization Solution in PyTorch)</strong></p>
<p>Since version 1.8, PyTorch launched the FX Graph tracing technique and relevant quantization solution.
Compared with the previous Eager Mode Solution, the FX Graph is an automatic, configurable solution with which the users dont’ need to modify the code.
More information about the FX Graph solution please refer to
<a class="reference external" href="https://pytorch.org/docs/stable/quantization.html">Quantization ‒ PyTorch 1.9.0 documentation</a>.
Presently this solution is popular among developers and is becoming more and more reliable.
Horizon’s Toolchain provides the capacity to deploy this solution.</p>
<p><strong>The QAT Solution Based on PyTorch FX Graph</strong></p>
<p>Users needs to call the <code class="docutils literal notranslate"><span class="pre">prepare_qat_fx</span></code> function of <code class="docutils literal notranslate"><span class="pre">torch</span></code>, specify corresponding node parameters,
in order to accomplish the conversion from floating-point to quantized model automatically.
Refer to below function API:</p>
<dl class="py data">
<dt class="sig sig-object py">
<span class="sig-name descname"><span class="pre">the</span> <span class="pre">prepare_qat_fx</span> <span class="pre">Function：</span></span></dt>
<dd><dl class="simple">
<dt><strong>PURPOSE:</strong></dt><dd><p>This function converts a floating-point model into a Prepare model</p>
</dd>
<dt><strong>PARAMETERS:</strong></dt><dd><dl class="simple">
<dt>model：</dt><dd><p>A model of the <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> class, whose state must be train.</p>
</dd>
<dt>qconfig_dict：</dt><dd><p>a quantization method declaring the quantization nodes, e.g. asymmetric, per-tensor etc.</p>
</dd>
<dt>prepare_custom_config_dict：</dt><dd><p>A option to specify the prepare process. For example, it can be used for specifying the layer which will not be quantized,
or specifying not to trace a certain layer using the FX. It also can be used for specifying that some structures (avgpooling+relu)
can be quantized by packaging.</p>
</dd>
</dl>
</dd>
<dt><strong>RETURN:</strong></dt><dd><p>It returns a prepare model which can be used for QAT.</p>
</dd>
</dl>
</dd></dl>

<p>When calling the <code class="docutils literal notranslate"><span class="pre">quantize_fx.prepare_qat_fx</span></code>, the following steps will be proceeded:</p>
<ul class="simple">
<li><p><strong>Create static graph</strong> use the fx tool to trace the entire network structure
(the <code class="docutils literal notranslate"><span class="pre">prepare_custum_config_dict</span></code> can be used for specifying not to trace a certain layer) and create a static network structure.</p></li>
<li><p><strong>Fuse specific network structure:</strong> load the default or the user-specified <code class="docutils literal notranslate"><span class="pre">fuse</span> <span class="pre">pattern</span></code>, in order to traverse, fuse network structure.
For example, PyTorch fuses the conv+bn+relu into <code class="docutils literal notranslate"><span class="pre">instrice.ConvBnRelu</span></code> by default.</p></li>
<li><p><strong>Convert network structure:</strong> load the default or user-specified <code class="docutils literal notranslate"><span class="pre">convert</span> <span class="pre">pattern</span></code>, in order to convert a specific network structure into
a specified network structure.</p></li>
<li><p><strong>Quantize network structure:</strong> load the default or user-specified <code class="docutils literal notranslate"><span class="pre">quantize</span> <span class="pre">pattern</span></code>, insert quantization aware nodes into specified positions,
such as, insert the <code class="docutils literal notranslate"><span class="pre">observer</span></code> after <code class="docutils literal notranslate"><span class="pre">quant.ConvBnRelu</span></code>.</p></li>
<li><p><strong>More processings.</strong></p></li>
</ul>
<p><strong>Load Horizon’s Qconfig based on PyTorch’s FX Graph QAT solution</strong></p>
<p>Because PyTorch’s default QAT method cannot be efficiently executed in Horizon’s AI ASICs,
Users can adjust the QAT configurations by loading Toolchain provided quantization parameters.</p>
<p>Below displays the calling flowchart (Toolchain provided APIs are in yellow):</p>
<figure class="align-center" id="id10">
<a class="reference internal image-reference" href="_images/qat_load_process.png"><img alt="_images/qat_load_process.png" src="_images/qat_load_process.png" style="width: 1077.0px; height: 177.0px;" /></a>
<figcaption>
<p><span class="caption-text">Single click to enlarge the image</span><a class="headerlink" href="#id10" title="Permalink to this image"></a></p>
</figcaption>
</figure>
<p>Users only need to import the <code class="docutils literal notranslate"><span class="pre">HorizonQConfig</span></code> from the <code class="docutils literal notranslate"><span class="pre">horizon_nn</span></code> to specify quantization strategy.
If users need high accuracy output (float32) from the last conv layer, please call the <code class="docutils literal notranslate"><span class="pre">adjust_qat_bits</span></code> to modify the model.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>The evaluation and training of QAT models are the same as that of floating-point models.
But note that by default, the quantization parameters of QAT model will be updated in each inference (training or eval),
so every time, the evaluation results can be different.
Users will need to manually modify the state of QAT model before trainig or evaluation,
in order to enable the QAT model to only update parameters during training.
Users can realize this by import the <code class="docutils literal notranslate"><span class="pre">set_qat_eval</span></code> and <code class="docutils literal notranslate"><span class="pre">set_qat_training</span></code> in the Toolchain.</p>
</div>
<p><strong>APIs</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># adjust your model nodes and quantization strategy</span>
<span class="k">def</span> <span class="nf">adjust_qat_bits</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span> <span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    desc:</span>
<span class="sd">        adjust the bits of some special nodes on qat model, like disable last fake quant</span>
<span class="sd">    :param model: torch qat model.</span>
<span class="sd">    :return: torch qat model after adjusted.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<span class="c1"># convert the QAT model into ONNX model, and store it in the OnnxModule</span>
<span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
            <span class="n">dummy_input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
            <span class="n">march</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bernoulli2&quot;</span><span class="p">,</span>
            <span class="o">*</span><span class="n">args</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OnnxModule</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">      desc:</span>
<span class="sd">          convert qat model into quantized model.</span>
<span class="sd">      :param model: torch qat model, onnx model, or onnx model path</span>
<span class="sd">      :param dummy_input: the example input, whose type could be dict, Tensor, list.</span>
<span class="sd">      :param march: on board march, default is bernoulli2, selected form[&#39;bernoulli2&#39;]</span>
<span class="sd">      :return: OnnxModule, which could run with torch dataloader as same as qat model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<span class="c1"># compile the ONNX model into heterogeneous model</span>
<span class="k">def</span> <span class="nf">compile</span><span class="p">(</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">OnnxModule</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
    <span class="n">dummy_input</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">output_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;model.bin&quot;</span><span class="p">,</span>
    <span class="n">march</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bernoulli2&quot;</span><span class="p">,</span>
    <span class="n">rt_input_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;yuv444&quot;</span><span class="p">,</span>
    <span class="n">rt_input_layout</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
    <span class="n">opt</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;O0&quot;</span><span class="p">,</span>
    <span class="n">debug</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        desc:</span>
<span class="sd">            compile quantized model to bin model.</span>
<span class="sd">        :param model: onnx model, or onnx model file path.</span>
<span class="sd">        :param dummy_input: tensor for dump onnx model from torch.nn.Module</span>
<span class="sd">        :param output_name: export bin model filename</span>
<span class="sd">        :param march: on board march, default is bernoulli2, selected form[&#39;bernoulli2&#39;]</span>
<span class="sd">        :param rt_input_type: runtime input type, selected from [nv12, gray, rgb, bgr, feature_map, yuv444].</span>
<span class="sd">        :param rt_input_layout: runtime input layout, selected from [NHWC, NCHW].</span>
<span class="sd">        :param opt: optimized level, select from [&#39;O0&#39;, &#39;O1&#39;, &#39;O2&#39;, &#39;O3&#39;], high level mean long time and better performance.</span>
<span class="sd">        :param debug: debug model allow user to dump all output.</span>

<span class="sd">        :return: None</span>
<span class="sd">    &quot;&quot;&quot;</span>
</pre></div>
</div>
<p><strong>A Complete Quantization Sample</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.quantization.quantize_fx</span> <span class="kn">import</span> <span class="n">prepare_qat_fx</span>
<span class="kn">from</span> <span class="nn">horizon_nn.torch</span> <span class="kn">import</span> <span class="n">HorizonQConfig</span><span class="p">,</span> <span class="n">adjust_qat_bits</span><span class="p">,</span> <span class="n">set_qat_eval</span><span class="p">,</span> <span class="n">set_qat_training</span><span class="p">,</span> <span class="n">export_to_onnx</span><span class="p">,</span> <span class="n">convert</span>
<span class="kn">from</span> <span class="nn">horizon_tc_ui.torch</span> <span class="kn">import</span> <span class="nb">compile</span>

<span class="k">def</span> <span class="nf">load_model</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">accuracy</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="k">pass</span>
<span class="k">def</span> <span class="nf">prepare_data_loaders</span><span class="p">():</span>
    <span class="k">pass</span>

<span class="n">data_loader</span> <span class="o">=</span> <span class="n">prepare_data_loaders</span><span class="p">()</span>
<span class="n">float_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span><span class="c1"># user trained model</span>

<span class="c1"># configure quantization strategy based on HorizonQConfig</span>
<span class="n">qat_model</span> <span class="o">=</span> <span class="n">prepare_qat_fx</span><span class="p">(</span><span class="n">float_model</span><span class="p">,</span> <span class="n">HorizonQConfig</span><span class="p">)</span>

<span class="c1"># Specify the last conv layer to dump high accuracy output (please skip this step if it is not required)</span>
<span class="n">qat_model</span> <span class="o">=</span> <span class="n">adjust_qat_bits</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>

<span class="k">for</span> <span class="n">nepoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epoch_size</span><span class="p">):</span>
    <span class="c1"># specify the model as a train model, enable quantization parameter update</span>
    <span class="n">qat_model</span> <span class="o">=</span> <span class="n">set_qat_training</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>
    <span class="n">train</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>

    <span class="c1"># specify model as eval model, disable quantzation parameter udpate</span>
    <span class="n">qat_model</span> <span class="o">=</span> <span class="n">set_qat_eval</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>
    <span class="n">top1</span><span class="p">,</span> <span class="n">top5</span> <span class="o">=</span> <span class="nb">eval</span><span class="p">(</span><span class="n">qat_model</span><span class="p">)</span>

<span class="c1"># store the trained model</span>
<span class="n">save_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;state_dict&#39;</span><span class="p">:</span><span class="n">qat_model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()}</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_dict</span><span class="p">,</span><span class="s2">&quot;qat_best.pth&quot;</span><span class="p">)</span>

<span class="c1"># export the QAT model as ONNX model</span>
<span class="n">dummy_data</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">export_to_onnx</span><span class="p">(</span><span class="n">qat_model</span><span class="p">,</span> <span class="n">dummy_data</span><span class="p">)</span>

<span class="c1"># load PyTorch model</span>
<span class="n">float_model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">()</span>
<span class="n">qat_model</span> <span class="o">=</span> <span class="n">prepare_fx_qat</span><span class="p">(</span><span class="n">float_model</span><span class="p">,</span> <span class="n">HorizonQConfig</span><span class="p">)</span>
<span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;qat_best.pth&#39;</span><span class="p">)[</span><span class="s1">&#39;state_dict&#39;</span><span class="p">]</span>
<span class="n">qat_model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

<span class="c1"># convert the QAT model into ONNX model</span>
<span class="n">dummy_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span>
    <span class="n">qat_model</span><span class="p">,</span>  <span class="c1"># qat model</span>
    <span class="n">dummy_data</span><span class="p">,</span>  <span class="c1"># dummy data, or real data, which is the input data to feed the qat model</span>
    <span class="n">march</span><span class="o">=</span><span class="s1">&#39;bernoulli2&#39;</span>  <span class="c1"># quantization march</span>
<span class="p">)</span>

<span class="c1"># convert the fixed-point ONNX model into heterogeneous BIN model</span>
<span class="nb">compile</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span>
        <span class="s2">&quot;test.bin&quot;</span><span class="p">,</span>
        <span class="n">march</span><span class="o">=</span><span class="s2">&quot;bernoulli2&quot;</span><span class="p">,</span>
        <span class="n">rt_input_type</span><span class="o">=</span><span class="s2">&quot;yuv444&quot;</span><span class="p">,</span>
        <span class="n">rt_input_layout</span><span class="o">=</span><span class="s2">&quot;NCHW&quot;</span><span class="p">,</span>
        <span class="n">opt</span><span class="o">=</span><span class="n">opt</span><span class="p">,</span>
        <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="supported-op-list-and-restrictions">
<span id="op-restrictions"></span><h2><span class="section-number">3.7. </span>Supported OP List And Restrictions<a class="headerlink" href="#supported-op-list-and-restrictions" title="Permalink to this headline"></a></h2>
<p>Please refer to the
<strong>supported_op_list_and_restrictions_release</strong> Excel table in the
<strong>horizon_xj3_open_explorer_${version}_${date}/ddk/doc/navigation/ai_toolchain/docs_cn/supported_op_list_and_restrictions/</strong>
directory.</p>
</section>
<section id="other-dev-tools-optional">
<span id="other-tools"></span><h2><span class="section-number">3.8. </span>Other Dev Tools (Optional)<a class="headerlink" href="#other-dev-tools-optional" title="Permalink to this headline"></a></h2>
<p>This section introduces those dev tools that have not been mentioned in the above regular workflow.
These dev tools have their own particular applications and please apply where appropriate.</p>
<section id="pack-multiple-bin-models-into-one-file">
<h3><span class="section-number">3.8.1. </span>Pack Multiple Bin Models Into One File<a class="headerlink" href="#pack-multiple-bin-models-into-one-file" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">hb_pack</span></code> model packing tool provides the ability to pack multiple bin models into one file.
During the application development stage, Horizon provides the packed models relevant interfaces for users to
deal with those application scenarios with multiple models.
Refer to below command-line of the <code class="docutils literal notranslate"><span class="pre">hb_pack</span></code> tool:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_pack -o dst_name.bin  to_pack_1.bin to_pack_2.bin
</pre></div>
</div>
<p>Specify the packed filename using the <code class="docutils literal notranslate"><span class="pre">-o</span></code> parameter.</p>
<p>Add the bin models your want to pack up at the end of the command and use space to separate the models.</p>
</section>
<section id="view-model-information">
<h3><span class="section-number">3.8.2. </span>View Model Information<a class="headerlink" href="#view-model-information" title="Permalink to this headline"></a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">hb_model_info</span></code> model information viewing tool provides the ability to view the model’s configuration parameters
at the model conversion stage. Refer to below command-line:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_model_info model_name.bin
</pre></div>
</div>
<p>Add model name behind the <code class="docutils literal notranslate"><span class="pre">hb_model_info</span></code> command. If you want to use packed models, remember to add the <code class="docutils literal notranslate"><span class="pre">-p</span></code> parameter
after the <code class="docutils literal notranslate"><span class="pre">hb_model_info</span></code> command. This tool should print some conversion environment information and your specified
configuration parameters in model conversion. Please refer to the:
<a class="reference internal" href="#makertbin"><span class="std std-ref">Convert The Model Using The hb_mapper makertbin Tool</span></a> section to learn more about the configuration
parameters in model conversion.</p>
</section>
<section id="modify-the-nodes-of-bin-models">
<h3><span class="section-number">3.8.3. </span>Modify The Nodes Of Bin Models<a class="headerlink" href="#modify-the-nodes-of-bin-models" title="Permalink to this headline"></a></h3>
<p>To pursue ultimate performance in those scenarios with enormous input size, the quantization and conversion of some input
can be fused into data pre-processing. In such cases, use the <code class="docutils literal notranslate"><span class="pre">hb_model_modifier</span></code> tool to remove these nodes. Refer to
below command-line:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>hb_model_modifier  bin_file  -r <span class="o">{</span>node_name<span class="o">}</span>
</pre></div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">-r</span></code> parameter to specify the name of node to be deleted.</p>
<p>Use the <code class="docutils literal notranslate"><span class="pre">-o</span></code> parameter to specify the filename of the new model generated after node deleting.</p>
<p>The node name to be deleted must be the same as that of in the bin model, please refer to the:
<strong>Bin Model Structure</strong> subsection in the <a class="reference internal" href="#hb-perf"><span class="std std-ref">Use The hb_perf Tool To Evaluate Model Performance</span></a> section
to view node names.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note that this operation can invalidate the aforementioned model input format requirement, hence normally it is NOT
recommended to use.</p>
</div>
</section>
</section>
<section id="faq">
<h2><span class="section-number">3.9. </span>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline"></a></h2>
<section id="how-to-understand-the-bpu-supported-and-cpu-supported-etc-op-supporting-modes-mentioned-in-the-op-restrictions">
<h3><span class="section-number">3.9.1. </span>How to understand the <cite>BPU supported</cite> and <cite>CPU supported</cite> etc. OP supporting modes mentioned in the OP Restrictions?<a class="headerlink" href="#how-to-understand-the-bpu-supported-and-cpu-supported-etc-op-supporting-modes-mentioned-in-the-op-restrictions" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>The BPU supported OPs refer to those OPs who can utilize BPU hardware accelerator.
There are 2 support modes, namely the direct support mode and the post-fusion indirect support mode.
The direct support mode means that BPU hardware and compiler’s compilation and optimization have provided the OPs with
some particular implementation. While the post-fusion indirect support mode means that to support by fusing or replacing
the original OPs during model conversion and optimization processes.</p></li>
<li><p>The CPU supported OPs refer to those OPs who are neither in BPU bottom compiler OP range,
nor can they utilize the BPU accelerator after OP fusing and replacing.
Because they can only be computed in CPU, they are referred to as the CPU supported OPs.</p></li>
</ul>
</section>
<section id="why-the-model-performance-in-dev-board-test-differ-from-that-of-in-benchmark">
<h3><span class="section-number">3.9.2. </span>Why the model performance in dev board test differ from that of in benchmark?<a class="headerlink" href="#why-the-model-performance-in-dev-board-test-differ-from-that-of-in-benchmark" title="Permalink to this headline"></a></h3>
<p>Below lists some causes:</p>
<ul class="simple">
<li><p>Due to DDR bandwidth difference. The default DDR parameter standards of different dev boards vary,
and that the DDR bandwidth can cause huge impact on the AI toolchain’s final performance.
The dev boards’ command prompt includes frequency information, presently there are 2 types of commonly-seen frequencies:
2,666 and 3,200.</p></li>
<li><p>Due to the version number mismatch between the AI toolchain and the system image.
Ideally, version numbers of the 2 should match.</p></li>
<li><p>Due to CPU underclocking. Presently, by default, the dev board will enable automatic underclocking after reboot,
to obtain better model performance, you’ll need to run the underclocking command in dev board:
<code class="docutils literal notranslate"><span class="pre">echo</span> <span class="pre">performance</span> <span class="pre">&gt;</span> <span class="pre">/sys/devices/system/cpu/cpufreq/policy0/scaling_governor</span></code>.</p></li>
</ul>
</section>
<section id="whether-symmetric-or-asymmetric-model-quantization-is-used-whether-can-support-16-bit-quantization">
<h3><span class="section-number">3.9.3. </span>Whether symmetric or asymmetric model quantization is used? Whether can support 16-bit quantization?<a class="headerlink" href="#whether-symmetric-or-asymmetric-model-quantization-is-used-whether-can-support-16-bit-quantization" title="Permalink to this headline"></a></h3>
<p>Presently the AI toolchain for the X3/J3 AI ASICs can only support symmetric quantization.
The number of digits in quantization is restricted to ASICs’ hardware design,
hence presently it can only support 8-bit quantization.
The upcoming J5 etc. ASICs will support FP16 or 4-bit etc. types of quantization methods.
To improve the quantization accuracy,
Horizon Robotics has dedicated to optimizing the calibration algorithms of symmetric quantization.
The Auto-Search based KL calibration algorithm and the quantile-based MAX calibration algorithm
are proposed to deeply optimize the 2 industrial mainstream calibration methods, KL and MAX.
These 2 optimized calibration algorithms can ensure the performance and universality of quantization accuracy,
and can also guarantee &lt; 1% accuracy loss when quantizing classic models.</p>
</section>
<section id="how-does-model-segmentation-affects-performance">
<h3><span class="section-number">3.9.4. </span>How does model segmentation affects performance?<a class="headerlink" href="#how-does-model-segmentation-affects-performance" title="Permalink to this headline"></a></h3>
<p>As previously described in the <a class="reference internal" href="#id4"><span class="std std-ref">Convert The Model</span></a> section,
Those OPs who cannot utilize the BPU accelerator will degrade and be computed in CPU.
In such case a model will be sliced into 2 segments, one of which will be computed in BPU, while the other in CPU.
Typically, the less the segments (normally within 5 segments) and the less computations in CPU,
the smaller will be the effect on model accuracy.
But of course, the most reliable way to evaluate the effect is still to use the method as described in the:
<a class="reference internal" href="#performance-evaluation"><span class="std std-ref">Model Performance Analysis And Optimization</span></a> section.
If those CPU OPs cause the model performance in dev board failing the expectation, then please refer to the:
<a class="reference internal" href="#model-performance-optimization"><span class="std std-ref">Model Performance Optimization</span></a> section to optimize the model or contact
Horizon for technical support.</p>
</section>
<section id="which-model-conversion-parameters-can-affect-model-s-final-performance">
<h3><span class="section-number">3.9.5. </span>Which model conversion parameters can affect model’s final performance?<a class="headerlink" href="#which-model-conversion-parameters-can-affect-model-s-final-performance" title="Permalink to this headline"></a></h3>
<p>Please refer to: <a class="reference internal" href="#performance-affecting-parameters"><span class="std std-ref">Check Those Performance-affecting YAML Configuration Parameters</span></a></p>
</section>
<section id="whether-or-not-support-sparsity-optimization">
<h3><span class="section-number">3.9.6. </span>Whether or not support sparsity optimization?<a class="headerlink" href="#whether-or-not-support-sparsity-optimization" title="Permalink to this headline"></a></h3>
<p>Sparsity refers to a technique to compress model parameters.
It can reduce model’s bandwidth usage in execution and further improve model execution efficiency.
However, the cost of which is to sacrifice algorithm accuracy after quantization.
Some AI acceleration schemes offer sparsity parameter to control compression rate,
so as to strike a balance between performance and accuracy.
Restricted by X3/J3 ASIC’s hardware, model sparsity conversion is not supported for now,
but we will support it in the future X5/J5 ASICs.
Yet in the existing X3/J3 ASICs, memory bandwidth usage has been significantly compressed by
combining the compiler hardware and software optimization.
In actual business scenarios, bandwidth usage reduction by sparsity is not needed.</p>
</section>
<section id="why-some-bpu-supported-ops-at-model-rear-part-run-in-cpu">
<h3><span class="section-number">3.9.7. </span>Why some BPU supported OPs at model rear part run in CPU?<a class="headerlink" href="#why-some-bpu-supported-ops-at-model-rear-part-run-in-cpu" title="Permalink to this headline"></a></h3>
<p>To begin with, we need to understand 2 below concepts:</p>
<ul class="simple">
<li><p>In the model conversion toolchain, presently only the Conv operator can support both int8 and int32 outputs,
while other OPs can only support the lower accuracy int8 output.</p></li>
<li><p>Normally, The BN and ReLU/ReLU6 after the Conv will be fused into Conv at model optimization stage.
But the higher accuracy int32 Conv is restricted to BPU hardware and therefore doesn’t support absorbing ReLU/ReLU6 OPs.</p></li>
</ul>
<p>Therefore, if the model is ended with Conv+ReLU/ReLU6, then to ensure the overall accuracy of the quantized model,
the Conv will by default use int32 output, while the ReLU/ReLU6 will run in CPU.
Similarly, the other rear-part OPs who run in CPU also because that the Conv OP needs higher-accuracy output.</p>
<dl class="footnote brackets">
<dt class="label" id="fpm"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>refers to the floating-point model users obtained from open source deep learning frameworks.</p>
</dd>
<dt class="label" id="hgm"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>refers to the heterogeneous model converted using <code class="docutils literal notranslate"><span class="pre">hb_mapper</span></code> tool.</p>
</dd>
<dt class="label" id="asic"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Application Specific Integrated Circuit.</p>
</dd>
<dt class="label" id="xj3"><span class="brackets">4</span></dt>
<dd><p>refers to the terms e.g. X3, J3, X/J3 or XJ3 who appear in this document multiple times. The J refers to Horizon’s Journey series ASICs which applies to the automobile industry; while the X refers to Horizon’s Sunrise series ASICs which applies to the IoT market. The digit comes after the letter represents product generation.</p>
</dd>
</dl>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="chapter_2_prerequisites.html" class="btn btn-neutral float-left" title="2. Prerequisites" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="chapter_4_application_development.html" class="btn btn-neutral float-right" title="4. Application Development" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Horizon Robotics.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>